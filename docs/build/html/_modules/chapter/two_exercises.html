

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>chapter.two_exercises &mdash; Machine Learning: A Probabilistic Perspective 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> Machine Learning: A Probabilistic Perspective
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../modules.html">chapter</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Machine Learning: A Probabilistic Perspective</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>chapter.two_exercises</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for chapter.two_exercises</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">random</span>

<div class="viewcode-block" id="question_1"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_1">[docs]</a><span class="k">def</span> <span class="nf">question_1</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    p(gender=boy) = 0.5</span>
<span class="sd">    p(gender=girl) = 0.5</span>

<span class="sd">    Possible outcomes of 2 children:</span>
<span class="sd">    boy, girl</span>
<span class="sd">    boy, boy</span>
<span class="sd">    girl, boy</span>
<span class="sd">    girl, girl</span>

<span class="sd">    a) If you know the neighbor has at least one boy, what is the probability the neighbor has a girl?</span>
<span class="sd">    Sample space: (b,g), (b,b), (g,b). 2/3 events have a girl involved, and they all have equal probability so 2/3.</span>

<span class="sd">    b) What is the probability that the other child is a girl if you see that one is a boy?</span>
<span class="sd">    Sample space: (b,g), (b,b). 1/2. The children are independent of each other, so it&#39;s the same as the probability</span>
<span class="sd">    of one child being a girl.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="question_2"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_2">[docs]</a><span class="k">def</span> <span class="nf">question_2</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    There is a blood found at a crime scene that has no innocent explanation. The blood is of a type found in</span>
<span class="sd">    1% of the population.</span>

<span class="sd">    a) Prosecutor&#39;s fallacy: 1% chance that the defendant would have the crime scene blood type if he was innocent,</span>
<span class="sd">    therefore there is a 99% chance that he is guilty.</span>

<span class="sd">    This is not what the evidence states: 1% of the population could have committed the crime because only they have</span>
<span class="sd">    the suspect blood type. The defendant has that blood type, so he is 1/K people who are in consideration for</span>
<span class="sd">    committing the crime, not 99% likely to have committed the crime. 99% of the population is not in consideration</span>
<span class="sd">    for the crime at all, but based on the blood evidence alone we cannot state the likelihood of this single</span>
<span class="sd">    defendent having committed this crime, only that he is in the consideration set.</span>

<span class="sd">    b) Defendant&#39;s fallacy: There are 800K people in the city, 8000 have the blood type in question. There is just</span>
<span class="sd">    1 in 8000 chance that the defendant is guilty and so has no relevance.</span>

<span class="sd">    While it is true that the defendant is just 1 of 8000 city dwellers that have the matching blood type, the blood</span>
<span class="sd">    is relevant. The true culprit must have that blood type, and so it establishes that further evidence must be</span>
<span class="sd">    produced to establish the innocence or guilt of the defendant. This is far from the situation that we can ignore</span>
<span class="sd">    the blood type, the guilty part(ies) must have that match to be considered for the crime.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="question_3"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_3">[docs]</a><span class="k">def</span> <span class="nf">question_3</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Variance of a sum.</span>

<span class="sd">    .. math::</span>
<span class="sd">        cov[X, Y] &amp;= \mathbb{E}[[X - \mathbb{E}[X]][Y - \mathbb{E}[Y]]]\\</span>
<span class="sd">            &amp;= \mathbb{E}[XY - X\mathbb{E}[Y] - Y\mathbb{E}[X] + \mathbb{E}[X]\mathbb{E}[Y]]\\</span>
<span class="sd">            &amp;= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[X]\mathbb{E}[Y] + \mathbb{E}[X]\mathbb{E}[Y]\\</span>
<span class="sd">            &amp;= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]</span>


<span class="sd">    .. math::</span>
<span class="sd">        var[X + Y] &amp;= \mathbb{E}[(X + Y - \mathbb{E}[X+Y])^2]\\</span>
<span class="sd">            &amp;= \mathbb{E}[X^2] + \mathbb{E}[XY] - \mathbb{E}[X\mathbb{E}[X+Y]] + \mathbb{E}[XY] + \mathbb{E}[Y^2] - \mathbb{E}[Y\mathbb{E}[X+Y]] - \mathbb{E}[X\mathbb{E}[X+Y]] - \mathbb{E}[Y\mathbb{E}[X+Y]] + \mathbb{E}[X+Y]^2\\</span>
<span class="sd">            &amp;= \mathbb{E}[X^2] - \mathbb{E}[X]^2 - \mathbb{E}[X]\mathbb{E}[Y] + \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 - \mathbb{E}[X]\mathbb{E}[Y] +2\mathbb{E}[XY] - \mathbb{E}[X]^2 - 2\mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[Y]^2 + \mathbb{E}[X+Y]^2\\</span>
<span class="sd">            &amp;= var(X) + var(Y) + 2\mathbb{E}[XY] - 4\mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[X]^2 - \mathbb{E}[Y]^2 + \mathbb{E}[X+Y]^2\\</span>
<span class="sd">            &amp;= var(X) + var(Y) + 2cov(X, Y) - 2\mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[X]^2 - \mathbb{E}[Y]^2 + \mathbb{E}[X]^2 + 2\mathbb{E}[X]\mathbb{E}[Y] + \mathbb{E}[Y]^2\\</span>
<span class="sd">            &amp;= var(X) + var(Y) + 2cov(X, Y)\\</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="question_4"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_4">[docs]</a><span class="k">def</span> <span class="nf">question_4</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given:</span>

<span class="sd">    .. math::</span>
<span class="sd">        P(T=p|D=p) &amp;= 0.99\\</span>
<span class="sd">        P(T=n|D=n) &amp;= 0.99\\</span>
<span class="sd">        P(D=p) &amp;= 1/10,000</span>


<span class="sd">    This is an application of Bayes Theorem since we want to update the prior probability of having</span>
<span class="sd">    the disease after knowing the test came back positive. So we have:</span>

<span class="sd">    .. math::</span>
<span class="sd">        P(D=p|T=p) &amp;= \frac{P(T=p|D=p) \cdot P(D=p)}{P(T=p)}, &amp;~\textrm{Bayes Thm.}\\</span>
<span class="sd">                   &amp;= \frac{P(T=p|D=p) \cdot P(D=p)}{\Sigma_d P(T=p|D=d)\cdot P(D=d)}, &amp;~\textrm{Law of Total Prob.}\\</span>
<span class="sd">                   &amp;= \frac{P(T=p|D=p) \cdot P(D=p)}{P(T=p|D=p) \cdot P(D=p) + P(T=p|D=n) \cdot P(D=n)}, &amp;~\textrm{Notation}\\</span>
<span class="sd">                   &amp;= \frac{0.99 \cdot 0.0001}{0.99 \cdot 0.0001 + 0.01 \cdot 0.9999}, &amp;~\textrm{Law of Total Prob.}\\</span>
<span class="sd">                   &amp;\approx 0.0098.</span>


<span class="sd">    This means that the good news is the probability of having the disease is still a little less than 1/100. Also,</span>
<span class="sd">    The second application of the Law of Total Probability is actually two applications:</span>

<span class="sd">    .. math::</span>
<span class="sd">        1 &amp;= P(D=p) + P(D=n)\\</span>
<span class="sd">        1 &amp;= P(T=p|D=p) + P(T=p|D=n)</span>


<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="mf">0.99</span><span class="o">*</span><span class="mf">0.0001</span><span class="o">/</span><span class="p">(</span><span class="mf">0.99</span><span class="o">*</span><span class="mf">0.0001</span><span class="o">+</span><span class="mf">0.01</span><span class="o">*</span><span class="mf">0.9999</span><span class="p">))</span>
    <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="question_5"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_5">[docs]</a><span class="k">def</span> <span class="nf">question_5</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1337</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; The Monty Hall Problem using Bayes theorem.</span>

<span class="sd">    We&#39;re interested in determining whether switching doors is better than sticking with the original.</span>

<span class="sd">    Let :math:`C \sim Unif(3)` be the random variable representing where the car (prize) is,</span>
<span class="sd">    :math:`F \sim Unif(3)` be the random variable</span>
<span class="sd">    representing the first selection made by the contestant, and :math:`O` be the random variable representing</span>
<span class="sd">    which door is opened after the first selection is made. This variable is deterministic when the first guess does</span>
<span class="sd">    not equal the prize value but has a choice otherwise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        P(F=P|O, P) &amp;= \frac{P(O|F=P) \cdot P(F=P)}{P(O|P=F)},~&amp;\textrm{Bayes Theorem}\\</span>
<span class="sd">                    &amp;= \frac{1/2 \cdot 1/3}{1/2},~&amp;\textrm{Counting}\\</span>
<span class="sd">                    &amp;= 1/3.\\</span>
<span class="sd">        P(F\neq P|O, P) &amp;= \frac{P(O|F\neq P) \cdot P(F\neq P)}{P(O|P\neq F)},~&amp;\textrm{Bayes Theorem}\\</span>
<span class="sd">                        &amp;= \frac{1 \cdot 2/3}{1},~&amp;\textrm{Counting}\\</span>
<span class="sd">                        &amp;= 2/3.</span>

<span class="sd">    So from this we see that our first guess has a 2/3 chance of being wrong given the open door, so switching would</span>
<span class="sd">    give us a 2/3 of being correct in that case. Additionally, by the Law of Total Probability, we could&#39;ve computed</span>
<span class="sd">    the chances of the first guess being correct (1/3) and taking the complement of that.</span>

<span class="sd">    Side-effect:</span>
<span class="sd">        This code runs a simulation of the Monty Hall Problem to compute the probabilities and prints the</span>
<span class="sd">        probability of being right when staying with the original choice or switching to the remaining door.</span>

<span class="sd">    Args:</span>
<span class="sd">         num_samples (int): the number of times to sample the distribution, must be positive.</span>
<span class="sd">         seed (int): the random seed to ensure repeatability.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
    <span class="n">stay</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">switch</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
        <span class="n">prize</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">first</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">prize</span> <span class="o">!=</span> <span class="n">first</span><span class="p">:</span>
            <span class="c1"># Trick: 3 - (0 + 1): 2; 3 - (0 + 2): 1; 3 - (1 + 2): 0.</span>
            <span class="n">open_door</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">-</span> <span class="p">(</span><span class="n">first</span> <span class="o">+</span> <span class="n">prize</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Trick: 1 + 0: 1, 2 + 0: 2; 1 + 1= 2, 1 + 2 = 0; 2 + 1 = 0, 2 + 2 = 1.</span>
            <span class="n">open_door</span> <span class="o">=</span> <span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">prize</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span>
        <span class="k">if</span> <span class="n">first</span> <span class="o">==</span> <span class="n">prize</span><span class="p">:</span>
            <span class="n">stay</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Trick: 0 + 1 = 2, 0 + 2 = 1, 1 + 0 = 2, 1 + 2 = 0, 2 + 1 = 0 2 + 0 = 1</span>
        <span class="n">second</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">-</span> <span class="p">(</span><span class="n">open_door</span> <span class="o">+</span> <span class="n">first</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">prize</span> <span class="o">==</span> <span class="n">second</span><span class="p">:</span>
            <span class="n">switch</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;Correct stay probability: {stay/num_samples*100:0.3f}%;&quot;</span>
          <span class="n">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Correct switch probability: {switch/num_samples*100:0.3f}%&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="question_6"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_6">[docs]</a><span class="k">def</span> <span class="nf">question_6</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Want to know if you can compute :math:`P(H|e_1,e_2)` with different givens.</span>

<span class="sd">    Let&#39;s look at what this formula looks like after rearranging.</span>

<span class="sd">    .. math::</span>
<span class="sd">        P(H|e_1,e_2) &amp;= \frac{P(e_1,e_2|H) \cdot P(H)}{P(e_1,e_2)},~&amp;\textrm{Bayes Thm.}\\</span>
<span class="sd">                     &amp;= \frac{P(e_1|H) \cdot P(e_2|H) \cdot P(H)}{P(e_1,e_2)},~&amp;\textrm{Def. of Cond. Ind.}\\</span>
<span class="sd">                     &amp;= \frac{P(e_1|H) \cdot P(e_2|H) \cdot P(H)}{\Sigma_h P(e_1,e_2|H) \cdot P(H)},~&amp;\textrm{Total Probability}\\</span>
<span class="sd">                     &amp;= \frac{P(e_1|H) \cdot P(e_2|H) \cdot P(H)}{\Sigma_h P(e_1|H)\cdot P(e_2|H) \cdot P(H)},~&amp;\textrm{Def. of Cond. Ind.}</span>


<span class="sd">    i.      :math:`P(e_1,e_2), P(H), P(e_1|H), P(e_2|H)`. This is sufficient from the second line above if we assume</span>
<span class="sd">            independence between the :math:`E` variables.</span>
<span class="sd">    ii.     :math:`P(e_1,e_2), P(H), P(e_1,e_2|H)`. This is sufficient from the first line above, a single</span>
<span class="sd">            applications of Bayes Theorem.</span>
<span class="sd">    iii.    :math:`P(e_1|H), P(e_2|H), P(H)`. This is sufficient from the last line, after applying the Law of total</span>
<span class="sd">            probability and Conditional Independence.</span>


<span class="sd">    So (ii) is the answer to part a), when we don&#39;t know anything about the relationship between :math:`E_1` and</span>
<span class="sd">    :math:`E_2`. All sets of givens are sufficient if we know the two variables are conditionally independent.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># What is an example of conditional independence:</span>
    <span class="c1"># https://en.wikipedia.org/wiki/Conditional_independence</span>
    <span class="c1"># P(R|Y) = 4/12, P(B|Y) = 6/12, P(R|Y)*P(B|Y) = 6/36 = 2/12 = P(R,B|Y)</span>
    <span class="c1"># P(!R|Y) = 8/12, P(!B|Y) = 6/12, P(!R|Y)*P(!B|Y) = 8/24 = 4/12 = P(!R,!B|Y)</span>
    <span class="c1"># P(!R|Y) = 8/12, P(B|Y) = 6/12, P(!R|Y)*P(B|Y) = 8/24 = 4/12 = P(!R,B|Y)</span>
    <span class="c1"># P(R|Y) = 4/12, P(!B|Y) = 6/12 P(R|Y)*P(!B|Y) = 6/36 = 2/12 = P(R,!B|Y)</span>
    <span class="c1"># So R \ind B | Y.</span>
    <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="question_7"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_7">[docs]</a><span class="k">def</span> <span class="nf">question_7</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Pairwise independence does not imply mutual independence.</span>

<span class="sd">    Mutual independence means that :math:`P(X_i|X_S) = P(X_i) \forall S \subseteq \{1,\ldots,n\}\setminus\{i\}`</span>
<span class="sd">    and so the joint distribution of :math:`P(X_{1:n}) = \prod_{i=1}^n P(X_i)`.</span>

<span class="sd">    So it would be enough to show that for 3 variables that are all pairwise independent that they are</span>
<span class="sd">    not mutually independent.</span>

<span class="sd">    Consider a 5x5 grid where one variable :math:`(X_1)` is true only along the bottom 5 squares, another is true only</span>
<span class="sd">    along the right side :math:`(X_2)`, and a third is true only along the main diagonal :math:`(X_3)`. The only overlap</span>
<span class="sd">    any variable has with any other is in the lower right corner square.</span>

<span class="sd">    .. math::</span>
<span class="sd">        P(X_1=T) &amp;= 5/25\\</span>
<span class="sd">        P(X_1=F) &amp;= 20/25\\</span>
<span class="sd">        P(X_1=T,X_2=T) &amp;= 1/25 = 5/25*5/25 = P(X_1=T)P(X_2=T)\\</span>
<span class="sd">        P(X_1=T,X_2=F) &amp;= 4/25 = 5/25*20/25 = P(X_1=T)P(X_2=F)\\</span>
<span class="sd">        P(X_1=F,X_2=T) &amp;= 4/25 = 20/25*5/25 = P(X_1=F)P(X_2=T)\\</span>
<span class="sd">        P(X_1=F,X_2=F) &amp;= 16/25 = 20/25*20/25 = P(X_1=F)P(X_2=F)\\</span>

<span class="sd">    In this way, we see that each pair of variable is conditionally independent. The question is if they are</span>
<span class="sd">    mutually independent. If they were, then :math:`P(X_1,X_2,X_3) = P(X_1)P(X_2)P(X_3)`, but we see for</span>
<span class="sd">    :math:`P(X_1=T,X_2=T,X_3=T) = 1/25` (the lower right corner), but :math:`P(X_1=T)P(X_2=T)P(X_3=T) = 1/125` so</span>
<span class="sd">    we see that being pairwise conditionally independent does not imply mutual independence.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="question_8"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_8">[docs]</a><span class="k">def</span> <span class="nf">question_8</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Conditional independence iff joint factorizes.</span>

<span class="sd">    Prove that :math:`p(x,y|z)=g(x,z)h(y,z)~\textrm{iff}~X \perp Y | Z.`</span>

<span class="sd">    First, let :math:`g(x,z) = p(x|z), h(y,z) = p(y|z)` since conditional probabilities</span>
<span class="sd">    are functions of random variables these are permissible definitions of :math:`g, h`.</span>

<span class="sd">    :math:`\textrm{The forward direction:}~X \perp Y | Z \Rightarrow p(x,y|z)=g(x,z)h(y,z).`</span>

<span class="sd">    .. math::</span>
<span class="sd">       p(x,y|z) &amp;= p(x|z)p(y|z),~&amp;\textrm{Def. of Cond. Ind.}\\</span>
<span class="sd">                &amp;= g(x,z)h(y,z),~&amp;\textrm{Defined above.}.</span>

<span class="sd">    Lemma: :math:`p(x|y,z) = p(x|z)~\textrm{if}~X \perp Y | Z.`</span>

<span class="sd">    Proof:</span>

<span class="sd">    .. math::</span>
<span class="sd">        p(x|y,z) &amp;= \frac{p(x,y,z)}{p(y,z)},~&amp;\textrm{Def. of Cond. Prob.}\\</span>
<span class="sd">                 &amp;= \frac{p(x,y|z)p(z)}{p(y|z)p(z)}~&amp;\textrm{Def. of Cond. Prob.}\\</span>
<span class="sd">                 &amp;= \frac{p(x|z)p(y|z)p(z)}{p(y|z)p(z)}~&amp;\textrm{Def. of Cond. Ind.}\\</span>
<span class="sd">                 &amp;= p(x|z).</span>

<span class="sd">    :math:`\textrm{The reverse direction:}~p(x,y|z)=g(x,z)h(y,z) \Rightarrow X \perp Y | Z.`</span>

<span class="sd">    .. math::</span>
<span class="sd">        p(x,y|z) &amp;= \frac{p(x,y,z)}{p(z)},~&amp;\textrm{Def. of Cond. Prob.}\\</span>
<span class="sd">                 &amp;= \frac{p(z)p(y|z)p(x|y,z)}{p(z)},~&amp;\textrm{Chain rule of prob.}\\</span>
<span class="sd">                 &amp;= p(y|z)p(x|z),~&amp;\textrm{By the above lemma, Def. Cond. Ind.}\\</span>
<span class="sd">                 &amp;= g(x,z)h(y,z),~&amp;\textrm{Defined above.}</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="question_9"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_9">[docs]</a><span class="k">def</span> <span class="nf">question_9</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Conditional independence statements...</span>

<span class="sd">    a) Does :math:`(X \perp W|Z,Y) \wedge (X \perp Y|Z) \Rightarrow (X \perp Y,W|Z)`? Yes.</span>

<span class="sd">        .. math::</span>
<span class="sd">            p(X,Y,W|Z) &amp;= \frac{p(X,Y,W,Z)}{p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\</span>
<span class="sd">                &amp;= \frac{p(X,W|Z,Y)p(Z,Y)}{p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\</span>
<span class="sd">                &amp;= \frac{p(X|Z,Y)p(W|Z,Y)p(Z,Y)}{p(Z)},~&amp;\textrm{First given; Def. Cond. Ind.}\\</span>
<span class="sd">                &amp;= \frac{p(X,Z,Y)p(W|Z,Y)p(Z,Y)}{p(Z,Y)p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\</span>
<span class="sd">                &amp;= \frac{p(X,Y|Z)p(Z)p(W|Z,Y)}{p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\</span>
<span class="sd">                &amp;= p(X|Z)p(Y|Z)p(W|Z,Y),~&amp;\textrm{Second given; Def. Cond. Ind.}\\</span>
<span class="sd">                &amp;= \frac{p(X|Z)p(Y|Z)p(W,Z,Y)}{p(Z,Y)},~&amp;\textrm{Def. Cond. Prob.}\\</span>
<span class="sd">                &amp;= \frac{p(X|Z)p(Y|Z)p(Y,W|Z)p(Z)}{p(Z,Y)},~&amp;\textrm{Def. Cond. Prob.}\\</span>
<span class="sd">                &amp;= \frac{p(X|Z)p(Y,Z)p(Y,W|Z)p(Z)}{p(Z,Y)p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\</span>
<span class="sd">                &amp;= p(X|Z)p(Y,W|Z).</span>

<span class="sd">    b) Does :math:`(X \perp Y|Z) \wedge (X \perp Y|W) \Rightarrow (X \perp Y|Z,W)?` No.</span>

<span class="sd">        If W and Z are describing the same event, then this is a true statement, but in general,</span>
<span class="sd">        it fails. If we construct another discrete example using a 4x4 grid where X is true along</span>
<span class="sd">        the bottom, Y is true along the right side, Z is true along the main diagonal and W is true</span>
<span class="sd">        in the bottom right corner, the top left corner, and along the minor diagonal in the middle two</span>
<span class="sd">        rows (not where Z is true), then we&#39;ll have a contradiction. We get the first two statements as</span>
<span class="sd">        being true, :math:`(X \perp Y |Z) \wedge (X \perp Y|W)`, but we&#39;ll find that :math:`p(X|W,Z) = p(Y|W,Z) = 1/2`</span>
<span class="sd">        while :math:`p(X,Y|W,Z) = 1/2` not 1/4, giving us a contradiction and allowing us to say that the</span>
<span class="sd">        result is not true.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="question_10"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_10">[docs]</a><span class="k">def</span> <span class="nf">question_10</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Derive the inverse gamma distribution.</span>

<span class="sd">    If :math:`X \sim Ga(a, b)`, and :math:`Y = 1/X`, show that :math:`Y \sim IG(a, b)`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        p_y(y) &amp;= p_x(x)\left|\frac{dy}{dx}\frac{1}{X}\right|\\</span>
<span class="sd">            &amp;= \frac{b^a}{\Gamma(a)}\left(\frac{1}{x}\right)^{a-1}e^{-b/x}x^{-2}\\</span>
<span class="sd">            &amp;= \frac{b^a}{\Gamma(a)}x^{-(a-1)}e^{-b/x}x^{-2}\\</span>
<span class="sd">            &amp;= \frac{b^a}{\Gamma(a)}x^{-(a+1)}e^{-b/x}\\</span>
<span class="sd">            &amp;= IG(a, b).</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="question_11"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_11">[docs]</a><span class="k">def</span> <span class="nf">question_11</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Derive the 1D Gaussian normalization constant.</span>

<span class="sd">    We&#39;re going to need to do a little u-substitution:</span>

<span class="sd">    .. math::</span>
<span class="sd">        u &amp;= \frac{r^2}{2\sigma^2}\\</span>
<span class="sd">        du &amp;= \frac{2r}{2\sigma^2}dr\\</span>
<span class="sd">        \frac{\sigma^2}{r}du &amp;= dr.</span>

<span class="sd">    .. math::</span>
<span class="sd">        Z^2 &amp;= \int_0^{2\pi}\int_0^{\infty}r exp\left(\frac{-r^2}{2\sigma^2}\right) dr d\theta\\</span>
<span class="sd">            &amp;= \int_0^{2\pi}\int_0^{\infty}r exp\left(\frac{-r^2}{2\sigma^2}\right) dr d\theta\\</span>
<span class="sd">            &amp;= \int_0^{2\pi}\int_0^{\infty}re^{-u} du\frac{\sigma^2}{r}d\theta\\</span>
<span class="sd">            &amp;= \sigma^2\int_0^{2\pi}\int_0^{\infty}e^{-u} du d\theta\\</span>
<span class="sd">            &amp;= \sigma^2\int_0^{2\pi} \left.-e^{-u}\right|_0^{\infty} d\theta\\</span>
<span class="sd">            &amp;= \sigma^2\int_0^{2\pi} 1 d\theta\\</span>
<span class="sd">            &amp;= \sigma^2\left.\theta\right|_0^{2\pi}\\</span>
<span class="sd">            &amp;= \sigma^2 2\pi\\</span>
<span class="sd">        Z &amp;= \sqrt{\sigma^2 2\pi}\\</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="question_12"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_12">[docs]</a><span class="k">def</span> <span class="nf">question_12</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Express I(X,Y) as entropy...</span>

<span class="sd">    .. math::</span>
<span class="sd">        I(X,Y) &amp;= \Sigma_x\Sigma_y p(x,y) \log\frac{p(x,y)}{p(x)p(y)}\\</span>
<span class="sd">            &amp;= \Sigma_x\Sigma_y p(x|y)p(y) \log\frac{p(x|y)p(y)}{p(x)p(y)}\\</span>
<span class="sd">            &amp;= \Sigma_x\Sigma_y p(x|y)p(y) \left[\log p(x|y) - \log p(x)\right]\\</span>
<span class="sd">            &amp;= \Sigma_x\Sigma_y p(x|y)p(y)\log p(x|y) - \Sigma_x\Sigma_y p(x|y)p(y)\log p(x)\\</span>
<span class="sd">            &amp;= \Sigma_y p(y) \Sigma_x p(x|y)\log p(x|y) - \Sigma_x \log p(x) \Sigma_y p(x|y)p(y)\\</span>
<span class="sd">            &amp;= -H(X|Y) - \Sigma_x \log p(x) \Sigma_y p(x|y)p(y),~&amp;\textrm{Def. of Cond. Entropy}\\</span>
<span class="sd">            &amp;= -H(X|Y) - \Sigma_x p(x)\log p(x),~&amp;\textrm{Law of Total Prob.}\\</span>
<span class="sd">            &amp;= -H(X|Y) + H(X),~&amp;\textrm{Def. of Cond. Entropy}\\</span>
<span class="sd">            &amp;= H(X) - H(X|Y).</span>

<span class="sd">    You could simply change the way you go from joint to conditional variables in first step of the proof.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="question_13"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_13">[docs]</a><span class="k">def</span> <span class="nf">question_13</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="question_14"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_14">[docs]</a><span class="k">def</span> <span class="nf">question_14</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Show that normalized mutual information is a type of correlation.</span>

<span class="sd">    :math:`r = 1-\frac{H(Y|X)}{H(X)}`.</span>

<span class="sd">    a) Show :math:`r = \frac{I(X,Y)}{H(X)}`:</span>

<span class="sd">        .. math::</span>
<span class="sd">            r &amp;= 1 - \frac{H(Y|X)}{H(X)}\\</span>
<span class="sd">                &amp;= \frac{H(X)}{H(X)} - \frac{H(Y|X)}{H(X)}\\</span>
<span class="sd">                &amp;= \frac{H(Y) - H(Y|X)}{H(X)},~&amp;\textrm{X and Y are identically distributed.}\\</span>
<span class="sd">                &amp;= \frac{I(X,Y)}{H(X)},~&amp;\textrm{From Q2.12}.</span>

<span class="sd">    b) Show :math:`0 \leq r \leq 1`. We need to minimize and maximize the numerator. It is minimized when</span>
<span class="sd">    the :math:`log\frac{p(x,y)}{p(x)p(y)}` is minimized, so:</span>

<span class="sd">        .. math::</span>
<span class="sd">            0 &amp;= \log\frac{p(x,y)}{p(x)p(y)}\\</span>
<span class="sd">            \log(p(x)p(y)) &amp;= \log p(x,y)\\</span>
<span class="sd">            \log(p(x)p(y)) &amp;= \log(p(x)p(y)),~&amp;X \perp Y.</span>

<span class="sd">        If this term is 0 (and it can be if :math:`X \perp Y`), then the numerator is 0 and :math:`r=0`. The</span>
<span class="sd">        numerator is maximized when :math:`X=Y`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            I(X,X) &amp;= \Sigma_x \Sigma_y p(x,x) \log\frac{p(x,x)}{p(x)p(x)}\\</span>
<span class="sd">                &amp;= \Sigma_x \Sigma_y p(x) \log\frac{1}{p(x)}\\</span>
<span class="sd">                &amp;= \Sigma_x p(x) \log\frac{1}{p(x)}\\</span>
<span class="sd">                &amp;= \Sigma_x p(x) \log 1 - \Sigma_x p(x) \log p(x)\\</span>
<span class="sd">                &amp;= 0 + H(X)\\.</span>

<span class="sd">        So we end up with :math:`\frac{H(X)}{H(X)} = 1`. So we&#39;ve seen the min and max and we have shown that</span>
<span class="sd">        :math:`0 \leq r \leq 1`.</span>

<span class="sd">    c) :math:`r = 0` when :math:`X \perp Y`.</span>

<span class="sd">    d) :math:`r = 1` when :math:`X = Y`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="question_16"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_16">[docs]</a><span class="k">def</span> <span class="nf">question_16</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Mean, median, and mode of the Beta distribution.</span>

<span class="sd">    a) Mean:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \mathbb{E}[X] &amp;= \int_0^1 x B(a,b)dx\\</span>
<span class="sd">                &amp;= \int_0^1 x\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}dx\\</span>
<span class="sd">                &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x^a(1-x)^{b-1}dx\\</span>
<span class="sd">                &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x^{(a+1)-1}-x^{a+b-1}dx\\</span>
<span class="sd">                &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}B(a+1,b),~&amp;\textrm{Integral form of Beta}\\</span>
<span class="sd">                &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+b+1)},~&amp;\textrm{Def. of Beta}\\</span>
<span class="sd">                &amp;= \frac{\Gamma(a+b)}{\Gamma(a)}\frac{a\Gamma(a)}{(a+b)\Gamma(a+b)},~&amp;\textrm{Def. of }\Gamma,\\</span>
<span class="sd">                &amp;= \frac{a}{(a+b)}.</span>

<span class="sd">    b) Mode:</span>
<span class="sd">        We&#39;re going to take the derivative, set to 0, and solve to see what we get...</span>

<span class="sd">        .. math::</span>
<span class="sd">            B(a, b) &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}dx\\</span>
<span class="sd">            0 &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left[(a-1)x^{a-2}(1-x)^{b-1}-x^{a-1}(b-1)(1-x)^{b-2}\right]\\</span>
<span class="sd">            0 &amp;= (a-1)(1-x)-x(b-1)\\</span>
<span class="sd">            0 &amp;= a-x-1-ax-xb+x\\</span>
<span class="sd">            ax+bx-2x &amp;= a-1\\</span>
<span class="sd">            x &amp;= \frac{a-1}{a+b-2}.</span>

<span class="sd">    c) Variance:</span>
<span class="sd">        Just going to use the standard formula and hope for the best!</span>

<span class="sd">        .. math::</span>
<span class="sd">            Var(B(a,b)) &amp;= \int_0^1\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left(x-\frac{a}{a+b}\right)^2 x^{a-1}(1-x)^{b-1}dx\\</span>
<span class="sd">                &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1\left(x^2-\frac{2xa}{a+b}+\frac{a^2}{(a+b)^2}\right) x^{a-1}(1-x)^{b-1}dx\\</span>
<span class="sd">                &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left[\int_0^1 x^{(a+2)-1}(1-x)^{b-1}dx -\frac{2a}{a+b}\int_0^1 x^{(a+1)-1}(1-x)^{b-1}dx+\frac{a^2}{(a+b)^2}\int_0^1 x^{a-1}(1-x)^{b-1}dx\right]\\</span>
<span class="sd">                &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left[B(a+2,b) -\frac{2a}{a+b}B(a+1,b)+\frac{a^2}{(a+b)^2}B(a,b)\right]\\</span>
<span class="sd">                &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left[\frac{\Gamma(a+2)\Gamma(b)}{\Gamma(a+b+2)} -\frac{2a}{a+b}\frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+b+1)}+\frac{a^2}{(a+b)^2}\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\right]\\</span>
<span class="sd">                &amp;= \frac{\Gamma(a+b)}{\Gamma(a)}\left[\frac{(a+1)a\Gamma(a)}{(a+b+1)(a+b)\Gamma(a+b)} -\frac{2a}{a+b}\frac{a\Gamma(a)}{(a+b)\Gamma(a+b)}+\frac{a^2}{(a+b)^2}\frac{\Gamma(a)}{\Gamma(a+b)}\right]\\</span>
<span class="sd">                &amp;= \frac{(a+1)a}{(a+b+1)(a+b)} -\frac{2a}{a+b}\frac{a}{(a+b)}+\frac{a^2}{(a+b)^2}\\</span>
<span class="sd">                &amp;= \frac{a^2+a}{(a+b+1)(a+b)} -\frac{2a^2}{(a+b)^2}+\frac{a^2}{(a+b)^2}\\</span>
<span class="sd">                &amp;= \frac{(a^2+a)(a+b) - (2a^2)(a+b+1) + a^2(a+b+1)}{(a+b+1)(a+b)^2}\\</span>
<span class="sd">                &amp;= \frac{a^3+a^2b+a^2+ab - 2a^3-2a^2b-2a^2 + a^3+a^2b+a^2}{(a+b+1)(a+b)^2}\\</span>
<span class="sd">                &amp;= \frac{ab}{(a+b+1)(a+b)^2}.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="question_17"><a class="viewcode-back" href="../../chapter.html#chapter.two_exercises.question_17">[docs]</a><span class="k">def</span> <span class="nf">question_17</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1337</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Expected value of the minimum of 2 uniformly distributed numbers...</span>

<span class="sd">    The trick here is figuring out how to express the max function over two variables...</span>
<span class="sd">    Assuming :math:`x_1,x_2 \sim Unif(0,1)`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \mathbb{E}[min(x_1,x_2)] &amp;= \int_0^1\int_0^1 x_2\mathbb{I}(x_2 \leq x_1) + x_1\mathbb{I}(x_1 &lt; x_2)dx_2 dx_1\\</span>
<span class="sd">            &amp;= \int_0^1\int_0^1 x_2\mathbb{I}(x_2 \leq x_1) dx_2 dx_1 + \int_0^1\int_0^1 x_1\mathbb{I}(x_1 &lt; x_2)dx_2 dx_1\\</span>
<span class="sd">            &amp;= \int_0^1\int_0^{x_1} x_2 dx_2 dx_1 + \int_0^1\int_0^{x_2} x_1dx_1 dx_2\\</span>
<span class="sd">            &amp;= 2\int_0^1\int_0^{x_1} x_2 dx_2 dx_1\\</span>
<span class="sd">            &amp;= 2 \frac{1}{2\cdot 3}\\</span>
<span class="sd">            &amp;= \frac{1}{3}.</span>

<span class="sd">    In general, if you have :math:`n` variables from this distribution you can find the expected value of the min</span>
<span class="sd">    as :math:`n!\int_0^1\cdot\int_0^{x_n}x_n dx_n\cdots dx_1 = \frac{1}{n+1}` if you&#39;re talking about the uniform.</span>

<span class="sd">    Also, as part of experimenting to get this solution, I also did a categorical case, which is very similar except</span>
<span class="sd">    you need to worry about the situation where the two variables are equal (which has 0 probability in the continuous</span>
<span class="sd">    case): :math:`\frac{2}{n^2}\sum_{i=1}^n\sum_{j=1}^i x_j - \sum_{i=1}^n x_i`, where :math:`n` is the number of elements</span>
<span class="sd">    in the space and they are in ascending order. I believe going from 2 to :math:`k`</span>
<span class="sd">    draws will be similar, replacing the numerator with :math:`k!` and the denominator with :math:`k`.</span>

<span class="sd">    Args:</span>
<span class="sd">        k (int): Number of draws from the distribution to compute the min over.</span>
<span class="sd">        trials (int): Number of random min samples to select before computing the expected value.</span>
<span class="sd">        seed (int): Random seed for reproducibility.</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: the expected value of the min of ``k`` uniformly distributed variables.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">min_samps</span> <span class="o">=</span> <span class="p">[</span><span class="nb">min</span><span class="p">([</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">)])</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trials</span><span class="p">)]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">min_samps</span><span class="p">)</span><span class="o">/</span><span class="n">trials</span></div>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">question_5</span><span class="p">()</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Steven Loscalzo.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'0.0.1',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>