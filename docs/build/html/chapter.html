

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>chapter package &mdash; Machine Learning: A Probabilistic Perspective 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="chapter" href="modules.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Machine Learning: A Probabilistic Perspective
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">chapter</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">chapter package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-chapter.one_exercises">chapter.one_exercises module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-chapter.three_exercises">chapter.three_exercises module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-chapter.two_exercises">chapter.two_exercises module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-chapter">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Machine Learning: A Probabilistic Perspective</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="modules.html">chapter</a> &raquo;</li>
        
      <li>chapter package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/chapter.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="chapter-package">
<h1>chapter package<a class="headerlink" href="#chapter-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-chapter.one_exercises">
<span id="chapter-one-exercises-module"></span><h2>chapter.one_exercises module<a class="headerlink" href="#module-chapter.one_exercises" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="chapter.one_exercises.FLANN_KNN">
<em class="property">class </em><code class="descclassname">chapter.one_exercises.</code><code class="descname">FLANN_KNN</code><span class="sig-paren">(</span><em>k</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#FLANN_KNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.FLANN_KNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="chapter.one_exercises.FLANN_KNN.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#FLANN_KNN.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.FLANN_KNN.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="chapter.one_exercises.FLANN_KNN.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#FLANN_KNN.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.FLANN_KNN.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="chapter.one_exercises.LinearKNN">
<em class="property">class </em><code class="descclassname">chapter.one_exercises.</code><code class="descname">LinearKNN</code><span class="sig-paren">(</span><em>k</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#LinearKNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.LinearKNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="chapter.one_exercises.LinearKNN.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#LinearKNN.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.LinearKNN.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="chapter.one_exercises.LinearKNN.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>batch_size=2048</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#LinearKNN.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.LinearKNN.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="chapter.one_exercises.download_if_needed">
<code class="descclassname">chapter.one_exercises.</code><code class="descname">download_if_needed</code><span class="sig-paren">(</span><em>url</em>, <em>local</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#download_if_needed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.download_if_needed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.one_exercises.question_1">
<code class="descclassname">chapter.one_exercises.</code><code class="descname">question_1</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#question_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.question_1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.one_exercises.question_2">
<code class="descclassname">chapter.one_exercises.</code><code class="descname">question_2</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#question_2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.question_2" title="Permalink to this definition">¶</a></dt>
<dd><p>This code has some external dependencies. Namely, it uses FLANN and the
python bindings for it. Unfortunately, the maintainer of those bindings
hasn’t fixed some compatibility issues so a pull request needs to be used
to allow it to work with python 3. Or, you can fix the few compatibillity
errors <a class="reference external" href="https://github.com/primetang/pyflann/issues/1">yourself</a> .</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/primetang/pyflann.git
<span class="nb">cd</span> pyflann
git fetch origin pull/7/head:python36
git checkout python36
python setup.py install
</pre></div>
</div>
<p>In the end, the speedup is quite drastic. As the sample size doubles
from 500 to 1000 to 2000 there is almost no change in speed for the FLANN
version; however, the time increases substantially for the Linear version.</p>
</dd></dl>

<dl class="function">
<dt id="chapter.one_exercises.question_3">
<code class="descclassname">chapter.one_exercises.</code><code class="descname">question_3</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#question_3"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.question_3" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.one_exercises.read_mnist_img">
<code class="descclassname">chapter.one_exercises.</code><code class="descname">read_mnist_img</code><span class="sig-paren">(</span><em>local</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#read_mnist_img"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.read_mnist_img" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.one_exercises.read_mnist_labels">
<code class="descclassname">chapter.one_exercises.</code><code class="descname">read_mnist_labels</code><span class="sig-paren">(</span><em>local</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#read_mnist_labels"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.read_mnist_labels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-chapter.three_exercises">
<span id="chapter-three-exercises-module"></span><h2>chapter.three_exercises module<a class="headerlink" href="#module-chapter.three_exercises" title="Permalink to this headline">¶</a></h2>
<p>Worked examples and exercises from Chapter 3.</p>
<dl class="class">
<dt id="chapter.three_exercises.BetaBinomial">
<em class="property">class </em><code class="descclassname">chapter.three_exercises.</code><code class="descname">BetaBinomial</code><span class="sig-paren">(</span><em>alpha_0</em>, <em>alpha_1</em>, <em>seed=1337</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#BetaBinomial"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.BetaBinomial" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="chapter.three_exercises.BetaBinomial.posterior">
<code class="descname">posterior</code><span class="sig-paren">(</span><em>samples</em>, <em>freq=100</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#BetaBinomial.posterior"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.BetaBinomial.posterior" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="chapter.three_exercises.BetaBinomial.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#BetaBinomial.sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.BetaBinomial.sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="chapter.three_exercises.Concept">
<em class="property">class </em><code class="descclassname">chapter.three_exercises.</code><code class="descname">Concept</code><span class="sig-paren">(</span><em>name</em>, <em>extension=[]</em>, <em>prior=-1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#Concept"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.Concept" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>This is a concept or hypothesis from the Numbers Game.</p>
<dl class="attribute">
<dt id="chapter.three_exercises.Concept.name">
<code class="descname">name</code><a class="headerlink" href="#chapter.three_exercises.Concept.name" title="Permalink to this definition">¶</a></dt>
<dd><p><em>str</em> – The common name of the concept.</p>
</dd></dl>

<dl class="attribute">
<dt id="chapter.three_exercises.Concept.extension">
<code class="descname">extension</code><a class="headerlink" href="#chapter.three_exercises.Concept.extension" title="Permalink to this definition">¶</a></dt>
<dd><p><em>list int</em> – The values in the event space this concept describes.</p>
</dd></dl>

<dl class="attribute">
<dt id="chapter.three_exercises.Concept.prior">
<code class="descname">prior</code><a class="headerlink" href="#chapter.three_exercises.Concept.prior" title="Permalink to this definition">¶</a></dt>
<dd><p><em>float</em> – The prior probability of this concept being selected.</p>
</dd></dl>

<dl class="method">
<dt id="chapter.three_exercises.Concept.likelihood">
<code class="descname">likelihood</code><span class="sig-paren">(</span><em>n_samples</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#Concept.likelihood"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.Concept.likelihood" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the <span class="math notranslate nohighlight">\(p(D|h) = \frac{p(D,h)}{p(h)}\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>n_samples</strong> (<em>int</em>) – the number of samples collected from the target concept.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><dl class="docutils">
<dt>the likelihood of the data given the hypothesis.</dt>
<dd>This comes from the strong sampling assumption.</dd>
</dl>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list float</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="chapter.three_exercises.NumberGame">
<em class="property">class </em><code class="descclassname">chapter.three_exercises.</code><code class="descname">NumberGame</code><span class="sig-paren">(</span><em>*</em>, <em>seed=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#NumberGame"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.NumberGame" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>The game as described in Chapter 3 but used by Joshua Tenenbaum in his PhD Thesis.</p>
<p>The game demonstrates that it is possible to select a correct hypothesis using only
positive examples from the target.</p>
<p>Note: It is critical that all the candidate concepts are distinct. This might seem obvious,
but there are often many ways to describe the same concept in language. For example, consider
<code class="docutils literal notranslate"><span class="pre">multiples</span> <span class="pre">of</span> <span class="pre">10</span></code> and <code class="docutils literal notranslate"><span class="pre">ends</span> <span class="pre">in</span> <span class="pre">0</span></code> for the space <span class="math notranslate nohighlight">\([1, 100]\)</span>. These are clearly different
ideas, but they have the same extension in the space. This means that if one is the target, the
other is equally likely. This causes the optimization to converge to 0.5 instead of 1.0, playing
havoc with the program.</p>
<dl class="attribute">
<dt id="chapter.three_exercises.NumberGame.concepts">
<code class="descname">concepts</code><a class="headerlink" href="#chapter.three_exercises.NumberGame.concepts" title="Permalink to this definition">¶</a></dt>
<dd><p><em>list Concept</em> – The possible concepts in the game.</p>
</dd></dl>

<dl class="attribute">
<dt id="chapter.three_exercises.NumberGame.active_concept">
<code class="descname">active_concept</code><a class="headerlink" href="#chapter.three_exercises.NumberGame.active_concept" title="Permalink to this definition">¶</a></dt>
<dd><p>(Concept): The current target of the game, selected at random from <code class="docutils literal notranslate"><span class="pre">concepts</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="chapter.three_exercises.NumberGame.plugin_distribution">
<code class="descname">plugin_distribution</code><span class="sig-paren">(</span><em>samples</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#NumberGame.plugin_distribution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.NumberGame.plugin_distribution" title="Permalink to this definition">¶</a></dt>
<dd><p>What is the probability that any point belongs to the target concept
given that we “plug in” the most likely concept a posteriori?</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>samples</strong> – the samples seen so far.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">List of the posterior probabilities using the plug-in estimator.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="chapter.three_exercises.NumberGame.post_predictive_distribution">
<code class="descname">post_predictive_distribution</code><span class="sig-paren">(</span><em>samples</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#NumberGame.post_predictive_distribution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.NumberGame.post_predictive_distribution" title="Permalink to this definition">¶</a></dt>
<dd><p>What is the probability that any point belongs to the
target concept given the data we’ve seen so far?</p>
<div class="math notranslate nohighlight">
\[p(\tilde{x} \in C|\mathcal{D}) = \Sigma_h p(y=1|\tilde{x},h)p(h|\mathcal{D}).\]</div>
<p>Where <span class="math notranslate nohighlight">\(\tilde{x}\)</span> is a future observation and <span class="math notranslate nohighlight">\(y=1\)</span> states that the
observation is consistent with the given concept.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>samples</strong> (<em>list int</em>) – the samples from the target concept we’ve observed so far.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the posterior predictive distribution at this time.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list float</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="chapter.three_exercises.NumberGame.posterior">
<code class="descname">posterior</code><span class="sig-paren">(</span><em>samples</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#NumberGame.posterior"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.NumberGame.posterior" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate the full posterior probability of all <code class="docutils literal notranslate"><span class="pre">concepts</span></code>.</p>
<p>Computes the posterior <span class="math notranslate nohighlight">\(p(C|\mathcal{D}) = \frac{p(\mathcal{D}|h)p(h)}{p(\mathcal{D})}\)</span>,
substituting all possible concepts for <span class="math notranslate nohighlight">\(h\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>(</strong><strong>list</strong> (<em>samples</em>) – int): All samples generated by the target concept so far.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the posterior probability of all <code class="docutils literal notranslate"><span class="pre">concepts</span></code>.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list float</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="chapter.three_exercises.NumberGame.sample_from_concept">
<code class="descname">sample_from_concept</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#NumberGame.sample_from_concept"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.NumberGame.sample_from_concept" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a single sample from the <code class="docutils literal notranslate"><span class="pre">active_concept</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a number from the active concept.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.bb_main">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">bb_main</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#bb_main"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.bb_main" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.likelihood_ratio">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">likelihood_ratio</code><span class="sig-paren">(</span><em>posteriors</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#likelihood_ratio"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.likelihood_ratio" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.numbers_main">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">numbers_main</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#numbers_main"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.numbers_main" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_1">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_1</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_1" title="Permalink to this definition">¶</a></dt>
<dd><p>Optimize the log likelihood of <span class="math notranslate nohighlight">\(p(\mathcal{D}|\theta) = \theta^{N_1}(1-\theta)^{N_0}\)</span>
to prove <span class="math notranslate nohighlight">\(\frac{N_1}{N}\)</span>, the MLE of the Bernoulli/binomial model.</p>
<div class="math notranslate nohighlight">
\[\begin{split}log(p(\mathcal{D}|\theta) &amp;= log(\theta^{N_1}(1-\theta)^{N_0})\\
                          &amp;= N_1 log(\theta)+ N_0 log(1-\theta)\\
                          &amp;= N_1 log(\theta)+ (N-N_1)log(1-\theta).\end{split}\]</div>
<p>Now, optimizing for <span class="math notranslate nohighlight">\(\theta\)</span> by taking the derivative of the above and setting it
equal to 0.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{d}{d\theta} [N_1 log(\theta)+ (N-N_1)log(1-\theta)] &amp;= \frac{N_1}{\theta} - \frac{N-N_1}{1-\theta}\\
0 &amp;= \frac{N_1}{\theta} - \frac{N-N_1}{1-\theta}\\
N_1(1-\theta) &amp;= (N-N_1)\theta\\
N_1 - N_1\theta &amp;= N\theta - N_1\theta\\
N_1 &amp;= N\theta\\
\frac{N_1}{N} &amp;= \theta\\\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_10">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_10</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_10"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_10" title="Permalink to this definition">¶</a></dt>
<dd><p>Taxicab hijinks.</p>
<p>You go to a city and see a taxi numbered 100. Can we figure out how many taxis there are in this city?</p>
<p>a) Assuming we start with a <span class="math notranslate nohighlight">\(Pareto(\theta,0,0)\)</span> distribution on the number, what’s the posterior after
seeing that first taxicab numbered 100?</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(\theta|\mathcal{D}) &amp;= Pareto(\theta|N+K,max(0,100))\\
    &amp;= Pareto(\theta|1,100)\end{split}\]</div>
<ol class="loweralpha" start="2">
<li><p class="first">Compute the posterior mean, mode, and median:</p>
<blockquote>
<div><ol class="lowerroman simple">
<li>mean = DNE, the rate parameter needs to be bigger.</li>
<li>mode = 100, we’ve only seen 1 data point!</li>
<li>median = 200…</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}P(\theta \leq 0.5) &amp;= \\
0.5 &amp;= \int_m^x km^k\theta^{-(k+1)}d\theta\\
    &amp;= 100\int_{100}^x \theta^{-2}d\theta\\
    &amp;= 100\left[-\theta^{-1}\rvert_{100}^x\right]\\
    &amp;= \frac{100}{100} - \frac{100}{x}\\
0.5 &amp;= \frac{100}{x}\\
x = 200.\end{split}\]</div>
</div></blockquote>
</li>
<li><p class="first">Derive an expression for the posterior predictive after <span class="math notranslate nohighlight">\(\mathcal{D}=\{100\}\)</span>.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}p(\mathcal{D'}|\mathcal{D},\alpha) &amp;= \int p(\mathcal{D'}|\theta)p(\theta|\mathcal{D},\alpha)d\theta\\
   &amp;= \int_c^{\infty} Unif(x|\theta)Pareto(\theta|N+K,m)d\theta\\
   &amp;= \int_c^{\infty}\theta^{-1}(N+K)m^{N+K}\theta^{-(N+K+1)}d\theta\\
   &amp;= (N+K)m^{N+K}\int_c^{\infty}\theta^{-(N+K+2)}d\theta\\
   &amp;= (N+K)m^{N+K}\left[\left.\frac{-1}{(N+K+1)\theta^{N+K+1}}\right|_c^{\infty}\right]\\
   &amp;= \frac{m^{N+K}}{c^{N+K+1}}.\end{split}\]</div>
<p>This is all predicated on <span class="math notranslate nohighlight">\(c = max(\mathcal{D'},m)\)</span>. We need to notice that the likelihood of a future
point falling into the existing range doesn’t need to stretch the max, so any future value less than the current
max will get equal probability of happening. If we want to predict the probability of a higher numbered taxi,
then we need to start the integral from that point forward given the evidence we’ve collected so far, so that
should be less likely than a uniform distribution up to that number (since we haven’t seen such a large value
before). Using this formula, we can see that:</p>
<ol class="lowerroman simple">
<li><span class="math notranslate nohighlight">\(p(x=50|\mathcal{D},\alpha) = \frac{1}{100}\)</span></li>
<li><span class="math notranslate nohighlight">\(p(x=100|\mathcal{D},\alpha) = \frac{1}{100}\)</span></li>
<li><span class="math notranslate nohighlight">\(p(x=150|\mathcal{D},\alpha) = \frac{1}{225}\)</span></li>
</ol>
</div></blockquote>
</li>
</ol>
<p>e) There aren’t an infinite number of taxis, so there should be a reasonable upper bound in the integral. The prior
should also be set to a more reasonable value because the 150 case seems unusually low.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_11">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_11</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_11"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_11" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian analysis of the exponential distribution.</p>
<ol class="loweralpha">
<li><p class="first">Derive the MLE of <span class="math notranslate nohighlight">\(Expon(x|\theta)\)</span>.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}p(x|\theta) &amp;= Expon(x|\theta)\\
    &amp;= \theta e^{-x_1\theta}\cdots\theta e^{-x_n\theta}\\
    &amp;= \theta^n e^{-\theta \sum_i x_i},~&amp;\textrm{Take the der. and set to 0}\\
0   &amp;= n\theta^{n-1}e^{-\theta \sum_i x_i} - \theta^n \sum_i x_i e^{-\theta \sum_i x_i}\\
\theta^n \sum_i x_i &amp;= n\theta^{n-1}\\
\frac{\sum_i x_i}{n} &amp;= \frac{1}{\theta}\\
\bar{x} &amp;= \frac{1}{\theta}\\
\frac{1}{\bar{x}} &amp;= \theta.\end{split}\]</div>
</div></blockquote>
</li>
<li><p class="first">Given 3 observations of <span class="math notranslate nohighlight">\(X, {5, 4, 6}\)</span>, what is the MLE of this data? <span class="math notranslate nohighlight">\(\theta = \frac{1}{5}\)</span>.</p>
</li>
</ol>
<p>c) An expert thinks <span class="math notranslate nohighlight">\(p(\theta) = Expon(\theta|\lambda)\)</span>. Choose the prior <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span> such that
<span class="math notranslate nohighlight">\(\mathbb{E}[\theta] = 1/3\)</span>. We can do the MLE route again, to see that <span class="math notranslate nohighlight">\(\theta = \frac{1}{\lambda}\)</span> so
we end up with <span class="math notranslate nohighlight">\(\hat{\lambda} = 3\)</span> to get the desired expected value of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<ol class="loweralpha" start="4">
<li><p class="first">What is the posterior, <span class="math notranslate nohighlight">\(p(\theta|\mathcal{D},\hat{\lambda})\)</span>?</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}p(\theta|\mathcal{D},\hat{\lambda}) &amp;= p(\theta)p(\mathcal{D}|\theta\hat{\lambda})\\
    &amp;\propto Expon(\theta|\hat{\lambda})Expon(x|\theta)\\
    &amp;= \theta e^{-\theta\hat{\lambda}}\theta^{n}e^{-\theta\sum_i x_i}\\
    &amp;= \theta^{n+1}e^{-\theta(\sum_i x_i + \hat{\lambda}}\\
    &amp;= Gamma(\theta|n+2, \sum_i x_i + \hat{\lambda}).\end{split}\]</div>
</div></blockquote>
</li>
</ol>
<p>e) The exponential prior is not conjugate to the exponential likelihood. It results in a Gamma, but it turns out
that the exponential we selected was just a special case of the Gamma distribution. In general, based on this
analysis, I would say that we used a <span class="math notranslate nohighlight">\(Gamma(\theta|2,\hat{\lambda})\)</span> prior and not a <span class="math notranslate nohighlight">\(Expon(\theta|\hat{\lambda})\)</span>
prior since the posterior distribution should be the same form as the prior if it was conjugate.</p>
<ol class="loweralpha simple" start="6">
<li>The posterior mean is <span class="math notranslate nohighlight">\(\frac{n+2}{\sum_i x_i + \hat{\lambda}}\)</span> based on the statistics of the Gamma dist.</li>
</ol>
<p>g) The MLE and the posterior mean differ because there wasn’t a prior involved in the MLE derivation. The prior
suggests that the rate of failure is somewhat shorter than we have observed so far, and this additional information
wasn’t available in just the likelihood alone. The posterior mean is probably more reasonable assuming the experts
can give a reaonable prior estimate from their experience of studying other machines produced by a similar process.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_12">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_12</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_12"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_12" title="Permalink to this definition">¶</a></dt>
<dd><p>Bernoulli MAP estimate with non-conjugate priors.</p>
<ol class="loweralpha">
<li><p class="first">What if you used a prior: <span class="math notranslate nohighlight">\(p(\theta) = 0.5 if \theta = 0.5, 0.5 if \theta = 0.4, 0 otherwise\)</span>.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}p(\theta|D) &amp;= p(\theta)p(D|\theta)\\
    &amp;= p(\theta)\binom{N}{N_1}\theta^{N_1}(1-\theta)^{N-N_1}\\
    &amp;= 0.5\binom{N}{N_1}0.5^{N_1}0.5^{N-N_1}+0.5\binom{N}{N_1}0.4^{N_1}0.6^{N-N_1}\\
    &amp;= 0.5\binom{N}{N_1}\left[0.5^{N}+0.4^{N_1}0.6^{N-N_1}\right].\end{split}\]</div>
</div></blockquote>
</li>
</ol>
<p>b) What if the true parameter is <span class="math notranslate nohighlight">\(\theta = 0.41\)</span>. Which prior works better? For small <span class="math notranslate nohighlight">\(N\)</span>, we’ll likely
find that the new prior is more accurate because it wants a <span class="math notranslate nohighlight">\(\theta\)</span> close to 0.45. Unfortunately, its
effect never really diminishes with increasing trials and so the data doesn’t really overwhelm it. The Beta prior
is more likely to have greater errors in the early stages (unless very specific parameters are selected), but
instead of blending two specific values of <span class="math notranslate nohighlight">\(\theta\)</span> in fixed amounts, the blending is a part of the data
collections and in the limit will converge to the true value.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_2">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_2</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_2" title="Permalink to this definition">¶</a></dt>
<dd><p>Show that:</p>
<div class="math notranslate nohighlight">
\[\frac{[(\alpha_1)\cdots(\alpha_1 + N_1 - 1)][(\alpha_0)\cdots(\alpha_0+N_0-1)]}{(\alpha)\cdots(\alpha+N-1)}\]</div>
<p>Can be reduced to:</p>
<div class="math notranslate nohighlight">
\[\frac{[\Gamma(\alpha_1+N_1)\Gamma(\alpha_0+N_0)]}{\Gamma(\alpha_1+\alpha_0+N)}\frac{\Gamma(\alpha_1+\alpha_0)}{\Gamma(\alpha_1)\Gamma(\alpha_0)}\]</div>
<p>Using <span class="math notranslate nohighlight">\((\alpha-1)! = \Gamma(\alpha)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{[(\alpha_1)\cdots(\alpha_1 + N_1 - 1)][(\alpha_0)\cdots(\alpha_0+N_0-1)]}{(\alpha)\cdots(\alpha+N-1)} &amp;=\\
\frac{[(\alpha_1)\cdots(\alpha_1 + N_1 - 1)][(\alpha_0)\cdots(\alpha_0+N_0-1)]}{(\alpha)\cdots(\alpha+N-1)}\cdot\frac{(\alpha-1)!}{(\alpha-1)!} &amp;=\\
\frac{[(\alpha_1)\cdots(\alpha_1 + N_1 - 1)][(\alpha_0)\cdots(\alpha_0+N_0-1)]}{(\alpha+N-1)!}\cdot\frac{(\alpha-1)!}{1} &amp;=~,~&amp;\textrm{Def. of factorial}\\
\frac{[(\alpha_1)\cdots(\alpha_1 + N_1 - 1)][(\alpha_0)\cdots(\alpha_0+N_0-1)]}{(\alpha+N-1)!}\cdot\frac{(\alpha-1)!(\alpha_1-1)!(\alpha_0-1)!}{(\alpha_1-1)!(\alpha_0-1)!} &amp;=\\
\frac{(\alpha_1 + N_1 - 1)!(\alpha_0+N_0-1)!}{(\alpha+N-1)!}\cdot\frac{(\alpha-1)!}{(\alpha_1-1)!(\alpha_0-1)!} &amp;=~,~&amp;\textrm{Def. of factorial}\\
\frac{\Gamma(\alpha_1 + N_1)\Gamma(\alpha_0+N_0)}{\Gamma(\alpha+N)}\cdot\frac{\Gamma(\alpha)}{\Gamma(\alpha_1)\Gamma(\alpha_0)} &amp;=~,~&amp;\textrm{By the given}\\
\frac{\Gamma(\alpha_1 + N_1)\Gamma(\alpha_0+N_0)}{\Gamma(\alpha_1+\alpha_0+N)}\cdot\frac{\Gamma(\alpha_1+\alpha_0)}{\Gamma(\alpha_1)\Gamma(\alpha_0)} &amp;=~,~&amp;\textrm{Def.}~\alpha = \alpha_1+\alpha_0\end{split}\]</div>
<p>So we see that even without appealing to the Beta distribution, we can by sheer counts of the probability of the
data occurring arrive at the marginal likelihood for the Beta-Bernoulli model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_3">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_3</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_3"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_3" title="Permalink to this definition">¶</a></dt>
<dd><p>Posterior predictive for Beta-Binomial model</p>
<p>Prove that <span class="math notranslate nohighlight">\(p(x|n, \mathcal{D})=\frac{B(x+\alpha_1',n-x+\alpha_0')}{B(\alpha_1',\alpha_0')}\binom{n}{x}\)</span>
reduces to <span class="math notranslate nohighlight">\(p(\tilde{x}=1|\mathcal{D})=\frac{\alpha_1'}{\alpha_1'+\alpha_0'}\)</span> when <span class="math notranslate nohighlight">\(n=1\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{B(x+\alpha_1',n-x+\alpha_0')}{B(\alpha_1',\alpha_0')}\binom{n}{x} &amp;=\\
\frac{B(x+\alpha_1',1-x+\alpha_0')}{B(\alpha_1',\alpha_0')}\binom{1}{x} &amp;=~&amp;,~\textrm{Given}\\
\frac{B(1+\alpha_1',1-1+\alpha_0')}{B(\alpha_1',\alpha_0')}\cdot 1 &amp;=~&amp;,~\textrm{Given}~x=1\\
\frac{\Gamma(1+\alpha_1')\Gamma(\alpha_0')\Gamma(\alpha_1'+\alpha_0')}{\Gamma(1+\alpha_1'+\alpha_0')\Gamma(\alpha_1')\Gamma(\alpha_0')} &amp;=~&amp;,~\textrm{Def. of}~Beta\\
\frac{\alpha_1'\Gamma(\alpha_1')\Gamma(\alpha_1'+\alpha_0')}{(\alpha_1'+\alpha_0')\Gamma(\alpha_1'+\alpha_0')\Gamma(\alpha_1')} &amp;=~&amp;,~\Gamma(a+1)=a\Gamma(a)\\
\frac{\alpha_1'}{\alpha_1'+\alpha_0'}.\end{split}\]</div>
<p>So we can see that after a single trial, the posterior predictive of getting a 1 in that trial is simply the rate
of getting a 1 as given by the prior, which makes sense because we haven’t yet observed any data.
:returns: None.</p>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_4">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_4</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_4"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_4" title="Permalink to this definition">¶</a></dt>
<dd><p>Simple mixture distribution.</p>
<p>Let’s say we tossed a fair coin 5 times and know that &lt; 3 heads appeared. Compute the posterior up to
normalization constant..</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(X &lt; 3 | \theta) &amp;= p(X=0 | \theta) + p(X=1 | \theta) + p(X=2|\theta),~&amp;\textrm{Union of mutually exclusive events.}\\
                  &amp;\propto B(\theta|1,1)Bin(0|\theta,5) + B(\theta|1,1)Bin(1|\theta,5) + B(\theta|1,1)Bin(2|\theta,5),~&amp;\textrm{Bayes law}\\
                  &amp;\propto B(\theta|1,6) + B(\theta|2,5) + B(\theta|3,4),~&amp;\textrm{Conjugate prior}.\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_5">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_5</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_5"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_5" title="Permalink to this definition">¶</a></dt>
<dd><p>Uninformative prior for log-odds ratio.</p>
<p>Let <span class="math notranslate nohighlight">\(\phi = \textrm{logit}(\theta) = log\frac{\theta}{1-\theta}\)</span>.
Show that if <span class="math notranslate nohighlight">\(p(\phi) \propto 1\)</span>, then <span class="math notranslate nohighlight">\(p(\theta) \propto Beta(\theta|0, 0)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(\phi) &amp;= p(\theta)\left\vert\frac{d \theta}{d \phi}\right\vert,~&amp;\textrm{Change of variables}\\
        &amp;= log\frac{\theta}{1-\theta}\left\vert\frac{d\theta}{d\phi}\right\vert,\\
        &amp;= log\theta - log(1-\theta)\left\vert\frac{d\theta}{d\phi}\right\vert,\\
        &amp;= \frac{1}{\theta} + \frac{1}{1-\theta},\\
        &amp;= \frac{\theta + 1 - \theta}{\theta(1-\theta)},\\
        &amp;= \frac{1}{\theta(1-\theta)},\\
        &amp;= \theta^{-1}(1-\theta)^{-1},\\
        &amp;= B(\theta|0, 0),~&amp;\textrm{Def. of Beta}.\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_6">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_6</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_6"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_6" title="Permalink to this definition">¶</a></dt>
<dd><p>MLE for the Poisson distribution.
<span class="math notranslate nohighlight">\(Poi(x|\lambda) = e^{-\lambda}\frac{\lambda^x}{x!}\)</span>. Derive the MLE.</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(\lambda|x_1,\ldots,x_n) &amp;= e^{-\lambda}\frac{\lambda^{x_1}}{x_1!}\cdots e^{-\lambda}\frac{\lambda^{x_n}}{x_n!},~&amp;X~\sim~Poi(\lambda)\\
    &amp;= e^{-n\lambda}\frac{\lambda^{x_1+\cdots+x_n}}{\prod_i^n x_i!}.\end{split}\]</div>
<p>Set take the derivative and set it equal to 0 to find the maximum:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{d}{d\lambda}p(\lambda|x_1,\ldots,x_n) &amp;= -ne^{-n\lambda}\frac{\lambda^{x_1+\cdots+x_n}}{\prod_i^n x_i!} + e^{-n\lambda}\frac{(x_1+\cdots+x_n)\lambda^{x_1+\cdots+x_n-1}}{\prod_i^n x_i!},\\
    ne^{-n\lambda}\frac{\lambda^{x_1+\cdots+x_n}}{\prod_i^n x_i!} &amp;= e^{-n\lambda}\frac{(x_1+\cdots+x_n)\lambda^{x_1+\cdots+x_n-1}}{\prod_i^n x_i!},\\
    n\lambda^{x_1+\cdots+x_n} &amp;= (x_1+\cdots+x_n)\lambda^{x_1+\cdots+x_n-1},\\
    n\lambda &amp;= \sum_i^n x_i,\\
    \lambda &amp;= \frac{1}{n}\sum_i^n x_i.\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_7">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_7</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_7"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_7" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian derivation of Poisson MLE.</p>
<ol class="loweralpha simple">
<li>Derive the posterior assuming a conjugate prior <span class="math notranslate nohighlight">\(p(\lambda) = Ga(\lambda|a,b) \propto \lambda^{a-1}e^{-\lambda b}\)</span>.</li>
</ol>
<p>From the above, the likelihood of the Poisson distribution is: <span class="math notranslate nohighlight">\(e^{-n\lambda}\frac{\lambda^{x_1+\cdots+x_n}}{\prod_i^n x_i!}\)</span>,
so we can write the posterior as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(\lambda|D) &amp;\propto \lambda^{a-1}e^{-\lambda b}e^{-n\lambda}\lambda^{x_1+\cdots+x_n}\\
    &amp;= e^{-\lambda b -\lambda n}\lambda^{a+x_1+\cdots+x_n-1}\\
    &amp;= Ga(a+x_1+\cdots+x_n-1, b+n)\end{split}\]</div>
<ol class="loweralpha simple" start="2">
<li>The MLE of the posterior looks can be found:</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}0 &amp;= \frac{d}{d\lambda}e^{-\lambda b -\lambda n}\lambda^{a+x_1+\cdots+x_n-1}\\
  &amp;= -(b+n)e^{-\lambda(b+n)}\lambda^{a+x_1+\cdots+x_n-1}+e^{-\lambda b -\lambda n}(a+x_1+\cdots+x_n-1)\lambda^{a+x_1+\cdots+x_n-2}\\
(b+n)e^{-\lambda(b+n)}\lambda^{a+x_1+\cdots+x_n-1} &amp;= e^{-\lambda b -\lambda n}(a+x_1+\cdots+x_n-1)\lambda^{a+x_1+\cdots+x_n-2}\\
(b+n)\lambda^{a+x_1+\cdots+x_n-1} &amp;= (a+x_1+\cdots+x_n-1)\lambda^{a+x_1+\cdots+x_n-2}\\
(b+n)\lambda &amp;= a+x_1+\cdots+x_n-1\\
\lambda &amp;= \frac{a-1+\sum_i  x_i}{b+n}.\end{split}\]</div>
<p>If we look at what happens as the prior parameters <span class="math notranslate nohighlight">\(a, b \rightarrow 0\)</span>, we see that the mean of the posterior
approaches the MLE of the Poisson distribution.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_8">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_8</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_8"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_8" title="Permalink to this definition">¶</a></dt>
<dd><p>MLE for the uniform distribution.</p>
<ol class="loweralpha simple">
<li>What is the MLE for data <span class="math notranslate nohighlight">\(x_1,\ldots,x_n\)</span>?</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}p(a|x_1,\ldots,x_n) &amp;= \frac{\sum_i x_i}{(2a)^2}&amp;\\
    &amp;= \frac{-\sum_i x_i}{8a},~&amp;\textrm{Der. with respect to}~a\\
0   &amp;= \frac{-\sum_i x_i}{8a},~&amp;\textrm{Set equal to 0 to find maximum}.\end{split}\]</div>
<p>This is where some insight comes in. Solving for <span class="math notranslate nohighlight">\(a\)</span> doesn’t really work, but
if you think about plotting this function (for some fixed sum of data), you see that
it approaches 0 as <span class="math notranslate nohighlight">\(a\)</span> increases. The MLE occurs when <span class="math notranslate nohighlight">\(\hat{a} = max |x_i|, x_i \in {x_1,\ldots,x_n}\)</span>
since it captures all the data seen so far and there’s no support for anything further out
in magnitude on the number line.</p>
<ol class="loweralpha simple" start="2">
<li>The probability the model would assign to point <span class="math notranslate nohighlight">\(x_{n+1}\)</span> is:</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}p(x_{n+1}|\hat{a}) &amp;= \frac{1}{2\hat{a}},\\
    &amp;= \frac{1}{2x_{\textrm{max}}}.\end{split}\]</div>
<ol class="loweralpha simple" start="3">
<li>This doesn’t make a great deal of sense, especially if only a few data points have been observed.</li>
</ol>
<p>It states that
the only points that will be observed will be no bigger than <span class="math notranslate nohighlight">\(|x_{\textrm{max}}|\)</span>. In reality, the only thing
we know for sure is that <span class="math notranslate nohighlight">\(a\)</span> is at least that large. We may instead set a prior on <span class="math notranslate nohighlight">\(a\)</span> that takes into
account the range of feasible values based on the specific problem. It would have the effect of broadening the range
of values we expect to see, eventually tightening to all the points we’ve seen so far.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_9">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_9</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_9"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_9" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian analysis of the uniform dist.</p>
<p>Given a Pareto prior, the joint distribution of <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is
<span class="math notranslate nohighlight">\(p(\mathcal{D}, \theta) = \frac{Kb^K}{\theta^{N+K+1}}\mathbb{I}(\theta \geq max(\mathcal{D},b))\)</span>. We’re also
given <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span>, and are asked to derive the posterior <span class="math notranslate nohighlight">\(p(\theta|\mathcal{D})\)</span>. So…</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(\theta|\mathcal{D}) &amp;= \frac{p(\theta)p(\mathcal{D}|\theta)}{p(\mathcal{D})},~&amp;\textrm{Bayes rule}\\
    &amp;= \frac{p(\theta)p(\mathcal{D},\theta)}{p(\theta)p(\mathcal{D})},~&amp;\textrm{Def. of Cond Prob.}\\
    &amp;= \frac{p(\mathcal{D},\theta)}{p(\mathcal{D})}\\
    &amp;= \frac{Kb^K}{\theta^{N+K+1}}\cdot\frac{(N+K)m^{N+K}}{Kb^K},~&amp;\textrm{If max is }\geq b,~\textrm{or}\\
    &amp;= \frac{(N+K)m^{N+K}}{\theta^{N+K+1}},\\
    &amp;= \frac{Kb^K}{\theta^{N+K+1}}\cdot\frac{(N+K)b^{N}}{K},~&amp;\textrm{If max is }&lt; b,\\
    &amp;= \frac{(N+K)b^{N+K}}{\theta^{N+K+1}},\\
    &amp;= Pareto(\theta|N+K, max(\mathcal{D},b)),~&amp;\textrm{Def. of the Pareto distribution}.\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-chapter.two_exercises">
<span id="chapter-two-exercises-module"></span><h2>chapter.two_exercises module<a class="headerlink" href="#module-chapter.two_exercises" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="chapter.two_exercises.question_1">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_1</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_1" title="Permalink to this definition">¶</a></dt>
<dd><p>p(gender=boy) = 0.5
p(gender=girl) = 0.5</p>
<p>Possible outcomes of 2 children:
boy, girl
boy, boy
girl, boy
girl, girl</p>
<p>a) If you know the neighbor has at least one boy, what is the probability the neighbor has a girl?
Sample space: (b,g), (b,b), (g,b). 2/3 events have a girl involved, and they all have equal probability so 2/3.</p>
<p>b) What is the probability that the other child is a girl if you see that one is a boy?
Sample space: (b,g), (b,b). 1/2. The children are independent of each other, so it’s the same as the probability
of one child being a girl.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_2">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_2</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_2" title="Permalink to this definition">¶</a></dt>
<dd><p>There is a blood found at a crime scene that has no innocent explanation. The blood is of a type found in
1% of the population.</p>
<p>a) Prosecutor’s fallacy: 1% chance that the defendant would have the crime scene blood type if he was innocent,
therefore there is a 99% chance that he is guilty.</p>
<p>This is not what the evidence states: 1% of the population could have committed the crime because only they have
the suspect blood type. The defendant has that blood type, so he is 1/K people who are in consideration for
committing the crime, not 99% likely to have committed the crime. 99% of the population is not in consideration
for the crime at all, but based on the blood evidence alone we cannot state the likelihood of this single
defendent having committed this crime, only that he is in the consideration set.</p>
<p>b) Defendant’s fallacy: There are 800K people in the city, 8000 have the blood type in question. There is just
1 in 8000 chance that the defendant is guilty and so has no relevance.</p>
<p>While it is true that the defendant is just 1 of 8000 city dwellers that have the matching blood type, the blood
is relevant. The true culprit must have that blood type, and so it establishes that further evidence must be
produced to establish the innocence or guilt of the defendant. This is far from the situation that we can ignore
the blood type, the guilty part(ies) must have that match to be considered for the crime.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_3">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_3</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_3"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_3" title="Permalink to this definition">¶</a></dt>
<dd><p>Variance of a sum.</p>
<div class="math notranslate nohighlight">
\[\begin{split}cov[X, Y] &amp;= \mathbb{E}[[X - \mathbb{E}[X]][Y - \mathbb{E}[Y]]]\\
    &amp;= \mathbb{E}[XY - X\mathbb{E}[Y] - Y\mathbb{E}[X] + \mathbb{E}[X]\mathbb{E}[Y]]\\
    &amp;= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[X]\mathbb{E}[Y] + \mathbb{E}[X]\mathbb{E}[Y]\\
    &amp;= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}var[X + Y] &amp;= \mathbb{E}[(X + Y - \mathbb{E}[X+Y])^2]\\
    &amp;= \mathbb{E}[X^2] + \mathbb{E}[XY] - \mathbb{E}[X\mathbb{E}[X+Y]] + \mathbb{E}[XY] + \mathbb{E}[Y^2] - \mathbb{E}[Y\mathbb{E}[X+Y]] - \mathbb{E}[X\mathbb{E}[X+Y]] - \mathbb{E}[Y\mathbb{E}[X+Y]] + \mathbb{E}[X+Y]^2\\
    &amp;= \mathbb{E}[X^2] - \mathbb{E}[X]^2 - \mathbb{E}[X]\mathbb{E}[Y] + \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 - \mathbb{E}[X]\mathbb{E}[Y] +2\mathbb{E}[XY] - \mathbb{E}[X]^2 - 2\mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[Y]^2 + \mathbb{E}[X+Y]^2\\
    &amp;= var(X) + var(Y) + 2\mathbb{E}[XY] - 4\mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[X]^2 - \mathbb{E}[Y]^2 + \mathbb{E}[X+Y]^2\\
    &amp;= var(X) + var(Y) + 2cov(X, Y) - 2\mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[X]^2 - \mathbb{E}[Y]^2 + \mathbb{E}[X]^2 + 2\mathbb{E}[X]\mathbb{E}[Y] + \mathbb{E}[Y]^2\\
    &amp;= var(X) + var(Y) + 2cov(X, Y)\\\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_4">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_4</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_4"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_4" title="Permalink to this definition">¶</a></dt>
<dd><p>Given:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(T=p|D=p) &amp;= 0.99\\
P(T=n|D=n) &amp;= 0.99\\
P(D=p) &amp;= 1/10,000\end{split}\]</div>
<p>This is an application of Bayes Theorem since we want to update the prior probability of having
the disease after knowing the test came back positive. So we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(D=p|T=p) &amp;= \frac{P(T=p|D=p) \cdot P(D=p)}{P(T=p)}, &amp;~\textrm{Bayes Thm.}\\
           &amp;= \frac{P(T=p|D=p) \cdot P(D=p)}{\Sigma_d P(T=p|D=d)\cdot P(D=d)}, &amp;~\textrm{Law of Total Prob.}\\
           &amp;= \frac{P(T=p|D=p) \cdot P(D=p)}{P(T=p|D=p) \cdot P(D=p) + P(T=p|D=n) \cdot P(D=n)}, &amp;~\textrm{Notation}\\
           &amp;= \frac{0.99 \cdot 0.0001}{0.99 \cdot 0.0001 + 0.01 \cdot 0.9999}, &amp;~\textrm{Law of Total Prob.}\\
           &amp;\approx 0.0098.\end{split}\]</div>
<p>This means that the good news is the probability of having the disease is still a little less than 1/100. Also,
The second application of the Law of Total Probability is actually two applications:</p>
<div class="math notranslate nohighlight">
\[\begin{split}1 &amp;= P(D=p) + P(D=n)\\
1 &amp;= P(T=p|D=p) + P(T=p|D=n)\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_5">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_5</code><span class="sig-paren">(</span><em>num_samples=1000000</em>, <em>seed=1337</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_5"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_5" title="Permalink to this definition">¶</a></dt>
<dd><p>The Monty Hall Problem using Bayes theorem.</p>
<p>We’re interested in determining whether switching doors is better than sticking with the original.</p>
<p>Let <span class="math notranslate nohighlight">\(C \sim Unif(3)\)</span> be the random variable representing where the car (prize) is,
<span class="math notranslate nohighlight">\(F \sim Unif(3)\)</span> be the random variable
representing the first selection made by the contestant, and <span class="math notranslate nohighlight">\(O\)</span> be the random variable representing
which door is opened after the first selection is made. This variable is deterministic when the first guess does
not equal the prize value but has a choice otherwise.</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(F=P|O, P) &amp;= \frac{P(O|F=P) \cdot P(F=P)}{P(O|P=F)},~&amp;\textrm{Bayes Theorem}\\
            &amp;= \frac{1/2 \cdot 1/3}{1/2},~&amp;\textrm{Counting}\\
            &amp;= 1/3.\\
P(F\neq P|O, P) &amp;= \frac{P(O|F\neq P) \cdot P(F\neq P)}{P(O|P\neq F)},~&amp;\textrm{Bayes Theorem}\\
                &amp;= \frac{1 \cdot 2/3}{1},~&amp;\textrm{Counting}\\
                &amp;= 2/3.\end{split}\]</div>
<p>So from this we see that our first guess has a 2/3 chance of being wrong given the open door, so switching would
give us a 2/3 of being correct in that case. Additionally, by the Law of Total Probability, we could’ve computed
the chances of the first guess being correct (1/3) and taking the complement of that.</p>
<dl class="docutils">
<dt>Side-effect:</dt>
<dd>This code runs a simulation of the Monty Hall Problem to compute the probabilities and prints the
probability of being right when staying with the original choice or switching to the remaining door.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>num_samples</strong> (<em>int</em>) – the number of times to sample the distribution, must be positive.</li>
<li><strong>seed</strong> (<em>int</em>) – the random seed to ensure repeatability.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">None.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_6">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_6</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_6"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_6" title="Permalink to this definition">¶</a></dt>
<dd><p>Want to know if you can compute <span class="math notranslate nohighlight">\(P(H|e_1,e_2)\)</span> with different givens.</p>
<p>Let’s look at what this formula looks like after rearranging.</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(H|e_1,e_2) &amp;= \frac{P(e_1,e_2|H) \cdot P(H)}{P(e_1,e_2)},~&amp;\textrm{Bayes Thm.}\\
             &amp;= \frac{P(e_1|H) \cdot P(e_2|H) \cdot P(H)}{P(e_1,e_2)},~&amp;\textrm{Def. of Cond. Ind.}\\
             &amp;= \frac{P(e_1|H) \cdot P(e_2|H) \cdot P(H)}{\Sigma_h P(e_1,e_2|H) \cdot P(H)},~&amp;\textrm{Total Probability}\\
             &amp;= \frac{P(e_1|H) \cdot P(e_2|H) \cdot P(H)}{\Sigma_h P(e_1|H)\cdot P(e_2|H) \cdot P(H)},~&amp;\textrm{Def. of Cond. Ind.}\end{split}\]</div>
<ol class="lowerroman simple">
<li><span class="math notranslate nohighlight">\(P(e_1,e_2), P(H), P(e_1|H), P(e_2|H)\)</span>. This is sufficient from the second line above if we assume
independence between the <span class="math notranslate nohighlight">\(E\)</span> variables.</li>
<li><span class="math notranslate nohighlight">\(P(e_1,e_2), P(H), P(e_1,e_2|H)\)</span>. This is sufficient from the first line above, a single
applications of Bayes Theorem.</li>
<li><span class="math notranslate nohighlight">\(P(e_1|H), P(e_2|H), P(H)\)</span>. This is sufficient from the last line, after applying the Law of total
probability and Conditional Independence.</li>
</ol>
<p>So (ii) is the answer to part a), when we don’t know anything about the relationship between <span class="math notranslate nohighlight">\(E_1\)</span> and
<span class="math notranslate nohighlight">\(E_2\)</span>. All sets of givens are sufficient if we know the two variables are conditionally independent.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_7">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_7</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_7"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_7" title="Permalink to this definition">¶</a></dt>
<dd><p>Pairwise independence does not imply mutual independence.</p>
<p>Mutual independence means that <span class="math notranslate nohighlight">\(P(X_i|X_S) = P(X_i) \forall S \subseteq \{1,\ldots,n\}\setminus\{i\}\)</span>
and so the joint distribution of <span class="math notranslate nohighlight">\(P(X_{1:n}) = \prod_{i=1}^n P(X_i)\)</span>.</p>
<p>So it would be enough to show that for 3 variables that are all pairwise independent that they are
not mutually independent.</p>
<p>Consider a 5x5 grid where one variable <span class="math notranslate nohighlight">\((X_1)\)</span> is true only along the bottom 5 squares, another is true only
along the right side <span class="math notranslate nohighlight">\((X_2)\)</span>, and a third is true only along the main diagonal <span class="math notranslate nohighlight">\((X_3)\)</span>. The only overlap
any variable has with any other is in the lower right corner square.</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(X_1=T) &amp;= 5/25\\
P(X_1=F) &amp;= 20/25\\
P(X_1=T,X_2=T) &amp;= 1/25 = 5/25*5/25 = P(X_1=T)P(X_2=T)\\
P(X_1=T,X_2=F) &amp;= 4/25 = 5/25*20/25 = P(X_1=T)P(X_2=F)\\
P(X_1=F,X_2=T) &amp;= 4/25 = 20/25*5/25 = P(X_1=F)P(X_2=T)\\
P(X_1=F,X_2=F) &amp;= 16/25 = 20/25*20/25 = P(X_1=F)P(X_2=F)\\\end{split}\]</div>
<p>In this way, we see that each pair of variable is conditionally independent. The question is if they are
mutually independent. If they were, then <span class="math notranslate nohighlight">\(P(X_1,X_2,X_3) = P(X_1)P(X_2)P(X_3)\)</span>, but we see for
<span class="math notranslate nohighlight">\(P(X_1=T,X_2=T,X_3=T) = 1/25\)</span> (the lower right corner), but <span class="math notranslate nohighlight">\(P(X_1=T)P(X_2=T)P(X_3=T) = 1/125\)</span> so
we see that being pairwise conditionally independent does not imply mutual independence.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_8">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_8</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_8"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_8" title="Permalink to this definition">¶</a></dt>
<dd><p>Conditional independence iff joint factorizes.</p>
<p>Prove that <span class="math notranslate nohighlight">\(p(x,y|z)=g(x,z)h(y,z)~\textrm{iff}~X \perp Y | Z.\)</span></p>
<p>First, let <span class="math notranslate nohighlight">\(g(x,z) = p(x|z), h(y,z) = p(y|z)\)</span> since conditional probabilities
are functions of random variables these are permissible definitions of <span class="math notranslate nohighlight">\(g, h\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\textrm{The forward direction:}~X \perp Y | Z \Rightarrow p(x,y|z)=g(x,z)h(y,z).\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}p(x,y|z) &amp;= p(x|z)p(y|z),~&amp;\textrm{Def. of Cond. Ind.}\\
         &amp;= g(x,z)h(y,z),~&amp;\textrm{Defined above.}.\end{split}\]</div>
<p>Lemma: <span class="math notranslate nohighlight">\(p(x|y,z) = p(x|z)~\textrm{if}~X \perp Y | Z.\)</span></p>
<p>Proof:</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(x|y,z) &amp;= \frac{p(x,y,z)}{p(y,z)},~&amp;\textrm{Def. of Cond. Prob.}\\
         &amp;= \frac{p(x,y|z)p(z)}{p(y|z)p(z)}~&amp;\textrm{Def. of Cond. Prob.}\\
         &amp;= \frac{p(x|z)p(y|z)p(z)}{p(y|z)p(z)}~&amp;\textrm{Def. of Cond. Ind.}\\
         &amp;= p(x|z).\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\textrm{The reverse direction:}~p(x,y|z)=g(x,z)h(y,z) \Rightarrow X \perp Y | Z.\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}p(x,y|z) &amp;= \frac{p(x,y,z)}{p(z)},~&amp;\textrm{Def. of Cond. Prob.}\\
         &amp;= \frac{p(z)p(y|z)p(x|y,z)}{p(z)},~&amp;\textrm{Chain rule of prob.}\\
         &amp;= p(y|z)p(x|z),~&amp;\textrm{By the above lemma, Def. Cond. Ind.}\\
         &amp;= g(x,z)h(y,z),~&amp;\textrm{Defined above.}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_9">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_9</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_9"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_9" title="Permalink to this definition">¶</a></dt>
<dd><p>Conditional independence statements…</p>
<ol class="loweralpha">
<li><p class="first">Does <span class="math notranslate nohighlight">\((X \perp W|Z,Y) \wedge (X \perp Y|Z) \Rightarrow (X \perp Y,W|Z)\)</span>? Yes.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}p(X,Y,W|Z) &amp;= \frac{p(X,Y,W,Z)}{p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= \frac{p(X,W|Z,Y)p(Z,Y)}{p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= \frac{p(X|Z,Y)p(W|Z,Y)p(Z,Y)}{p(Z)},~&amp;\textrm{First given; Def. Cond. Ind.}\\
    &amp;= \frac{p(X,Z,Y)p(W|Z,Y)p(Z,Y)}{p(Z,Y)p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= \frac{p(X,Y|Z)p(Z)p(W|Z,Y)}{p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= p(X|Z)p(Y|Z)p(W|Z,Y),~&amp;\textrm{Second given; Def. Cond. Ind.}\\
    &amp;= \frac{p(X|Z)p(Y|Z)p(W,Z,Y)}{p(Z,Y)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= \frac{p(X|Z)p(Y|Z)p(Y,W|Z)p(Z)}{p(Z,Y)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= \frac{p(X|Z)p(Y,Z)p(Y,W|Z)p(Z)}{p(Z,Y)p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= p(X|Z)p(Y,W|Z).\end{split}\]</div>
</div></blockquote>
</li>
<li><p class="first">Does <span class="math notranslate nohighlight">\((X \perp Y|Z) \wedge (X \perp Y|W) \Rightarrow (X \perp Y|Z,W)?\)</span> No.</p>
<blockquote>
<div><p>If W and Z are describing the same event, then this is a true statement, but in general,
it fails. If we construct another discrete example using a 4x4 grid where X is true along
the bottom, Y is true along the right side, Z is true along the main diagonal and W is true
in the bottom right corner, the top left corner, and along the minor diagonal in the middle two
rows (not where Z is true), then we’ll have a contradiction. We get the first two statements as
being true, <span class="math notranslate nohighlight">\((X \perp Y |Z) \wedge (X \perp Y|W)\)</span>, but we’ll find that <span class="math notranslate nohighlight">\(p(X|W,Z) = p(Y|W,Z) = 1/2\)</span>
while <span class="math notranslate nohighlight">\(p(X,Y|W,Z) = 1/2\)</span> not 1/4, giving us a contradiction and allowing us to say that the
result is not true.</p>
</div></blockquote>
</li>
</ol>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-chapter">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-chapter" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="modules.html" class="btn btn-neutral" title="chapter" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Steven Loscalzo.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.0.1',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>