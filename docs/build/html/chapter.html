

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>chapter package &mdash; Machine Learning: A Probabilistic Perspective 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="chapter" href="modules.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Machine Learning: A Probabilistic Perspective
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">chapter</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">chapter package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-chapter.five_exercises">chapter.five_exercises module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-chapter.four">chapter.four module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-chapter.four_exercises">chapter.four_exercises module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-chapter.one_exercises">chapter.one_exercises module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-chapter.six">chapter.six module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-chapter.six_exercises">chapter.six_exercises module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-chapter.three_exercises">chapter.three_exercises module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-chapter.two_exercises">chapter.two_exercises module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-chapter">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Machine Learning: A Probabilistic Perspective</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="modules.html">chapter</a> &raquo;</li>
        
      <li>chapter package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/chapter.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="chapter-package">
<h1>chapter package<a class="headerlink" href="#chapter-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-chapter.five_exercises">
<span id="chapter-five-exercises-module"></span><h2>chapter.five_exercises module<a class="headerlink" href="#module-chapter.five_exercises" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="chapter.five_exercises.question_1">
<code class="descclassname">chapter.five_exercises.</code><code class="descname">question_1</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/five_exercises.html#question_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.five_exercises.question_1" title="Permalink to this definition">¶</a></dt>
<dd><p>Posterior of a mixture of conjugate priors is a mixture of conjugates.</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(\theta|D) &amp;= \frac{p(\theta, D)}{p(D)},~&amp;\textrm{Def. Cond. Prob},\\
    &amp;= \frac{\sum_k p(z=k)p(\theta, D | z=k)}{\sum_{k'} p(z=k')p(D|z=k')},~&amp;\textrm{Law of Total Prob},\\
    &amp;= \frac{\sum_k p(z=k)p(D|z=k)p(\theta|D,z=k)}{\sum_{k'} p(z=k')p(D|z=k')},~&amp;\textrm{Chain Rule of Prob.},\\
    &amp;= \sum_k p(z=k|D)p(\theta|D,z=k),~&amp;\textrm{Bayes Thm.}\end{split}\]</div>
<p>So we have the posterior of a mixture equal to the weighted sum of the posterior of each individual posterior, with
the weights being the posteriors of the mixture distribution.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.five_exercises.question_3">
<code class="descclassname">chapter.five_exercises.</code><code class="descname">question_3</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/five_exercises.html#question_3"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.five_exercises.question_3" title="Permalink to this definition">¶</a></dt>
<dd><p>Reject options.</p>
<p>Basically, you have C possible classes that you can select as your prediction (as defined by a particular problem)
plus another option: you can reject making a prediction. The cost of a misprediction is <span class="math notranslate nohighlight">\(\lambda_s\)</span>, while
the cost for rejecting is <span class="math notranslate nohighlight">\(\lambda_r\)</span>.</p>
<ol class="loweralpha">
<li><dl class="first docutils">
<dt>Figure out the minimum risk attained given the above parameters.</dt>
<dd><p class="first">We were given <span class="math notranslate nohighlight">\(p(y=j|x) = \theta\)</span>, but the trick is to figure out where the threshold <span class="math notranslate nohighlight">\(T\)</span> is between
getting a rejection and misprediction error. That probability is <span class="math notranslate nohighlight">\(p(\hat{y}=j|x) &gt; T\)</span>, so we can state
the loss function in terms of <span class="math notranslate nohighlight">\(T\)</span> and go from there. Make sure you pay attention to <span class="math notranslate nohighlight">\(\hat{y}\)</span> versus
<span class="math notranslate nohighlight">\(y\)</span>! Once the expected value expression is available, we take the derivative and set it equal to 0 to
find where it is maximized.</p>
<div class="last math notranslate nohighlight">
\[\begin{split}\mathbb{E}[\ell p(\hat{y}=j|x)] &amp;= \int_0^T \lambda_r t dt+ \int_T^1 \lambda_s (1-\theta) t dt,~\textrm{Def. of Expected Value}\\
    &amp;= \frac{\lambda_rT^2}{2} + \lambda_s(1-\theta)\left[\frac{1}{2} - \frac{T^2}{2}\right]\\
0 &amp;= \frac{\lambda_rT^2}{2} + \lambda_s(1-\theta)\left[\frac{1}{2} - \frac{T^2}{2}\right]\frac{d}{dT}\\
    &amp;= \lambda_rT - \lambda_s(1-\theta)T\\
    \frac{\lambda_rT}{\lambda_sT} &amp;= 1-\theta\\
    1-\frac{\lambda_r}{\lambda_s} &amp;= \theta.\end{split}\]</div>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Qualitatively, we see a linear function in terms of the cost matrix and the max class <span class="math notranslate nohighlight">\(j\)</span>.</dt>
<dd><p class="first last">As loss due to rejection grows from 0 to the cost of a misprediction error, we see at first that
the system must be 100% confident in its prediction to go forward with it, otherwise it’s free to reject. when
the two equal each other then there is no benefit to rejecting anything anymore and the majority prediction will
be selected at all times instead of the reject option.</p>
</dd>
</dl>
</li>
</ol>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.five_exercises.question_4">
<code class="descclassname">chapter.five_exercises.</code><code class="descname">question_4</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/five_exercises.html#question_4"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.five_exercises.question_4" title="Permalink to this definition">¶</a></dt>
<dd><p>More reject options.</p>
<p>Based on question 3 we know that the risk threshold when a reject option is in play is <span class="math notranslate nohighlight">\(1-\frac{\lambda_r}{\lambda_s}\)</span>,
and given the table in this problem, we see that it is a safer decision to reject a prediction if it is less than
<span class="math notranslate nohighlight">\(1-\frac{3}{10} = 0.7\)</span>. Using this fact we can solve the next couple of questions:</p>
<ol class="loweralpha simple">
<li><dl class="first docutils">
<dt>Suppose <span class="math notranslate nohighlight">\(p(y=1|x) = 0.2\)</span>. Which decision minimizes the expected loss? <span class="math notranslate nohighlight">\(\hat{y} = 0\)</span></dt>
<dd>Its probability is 0.8.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Suppose <span class="math notranslate nohighlight">\(p(y=1|x) = 0.4\)</span>. Which decision minimizes the expected loss? Reject.</dt>
<dd>The maximum confidence is 0.6 &lt; 0.7.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>The thresholds can be specified a little more crisply than the previous question where there were C classes.</dt>
<dd>In the binary case, predict 0 if <span class="math notranslate nohighlight">\(p_1 &lt; \frac{\lambda_r}{\lambda_s} = \theta_0\)</span>, predict 1 if
<span class="math notranslate nohighlight">\(p_1 &gt; 1-\frac{\lambda_r}{\lambda_s} = \theta_1\)</span>, and reject if <span class="math notranslate nohighlight">\(\theta_0 \leq p_1 \leq \theta_1\)</span>.
The idea is that any posterior distribution is a distribution and therefore sums to one. If <span class="math notranslate nohighlight">\(p_1\)</span> is
small, then <span class="math notranslate nohighlight">\(p_0\)</span> is large and we should select that, and vice versa. Maximum uncertainty occurs when
<span class="math notranslate nohighlight">\(p_1=p_0=0.5\)</span> in which case we should reject as long as the cost for doing so is less than the cost of
a misprediction. The reject region grows wider (symmetrically around 0.5 if the loss is equal for a misprediction
in either class) as the cost of a rejection grows smaller proportional to the cost of a misprediction.</dd>
</dl>
</li>
</ol>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-chapter.four">
<span id="chapter-four-module"></span><h2>chapter.four module<a class="headerlink" href="#module-chapter.four" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="chapter.four.LinearDiscriminantAnalysis">
<em class="property">class </em><code class="descclassname">chapter.four.</code><code class="descname">LinearDiscriminantAnalysis</code><a class="reference internal" href="_modules/chapter/four.html#LinearDiscriminantAnalysis"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.four.LinearDiscriminantAnalysis" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="chapter.four.LinearDiscriminantAnalysis.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>data</em>, <em>label</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/four.html#LinearDiscriminantAnalysis.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.four.LinearDiscriminantAnalysis.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="chapter.four.LinearDiscriminantAnalysis.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>data</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/four.html#LinearDiscriminantAnalysis.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.four.LinearDiscriminantAnalysis.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="chapter.four.QuadraticDiscriminantAnalysis">
<em class="property">class </em><code class="descclassname">chapter.four.</code><code class="descname">QuadraticDiscriminantAnalysis</code><a class="reference internal" href="_modules/chapter/four.html#QuadraticDiscriminantAnalysis"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.four.QuadraticDiscriminantAnalysis" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="chapter.four.QuadraticDiscriminantAnalysis.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>data</em>, <em>label</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/four.html#QuadraticDiscriminantAnalysis.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.four.QuadraticDiscriminantAnalysis.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="chapter.four.QuadraticDiscriminantAnalysis.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>data</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/four.html#QuadraticDiscriminantAnalysis.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.four.QuadraticDiscriminantAnalysis.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="chapter.four.decision_boundary">
<code class="descclassname">chapter.four.</code><code class="descname">decision_boundary</code><span class="sig-paren">(</span><em>model</em>, <em>data</em>, <em>target</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/four.html#decision_boundary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.four.decision_boundary" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-chapter.four_exercises">
<span id="chapter-four-exercises-module"></span><h2>chapter.four_exercises module<a class="headerlink" href="#module-chapter.four_exercises" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="chapter.four_exercises.question_17">
<code class="descclassname">chapter.four_exercises.</code><code class="descname">question_17</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/four_exercises.html#question_17"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.four_exercises.question_17" title="Permalink to this definition">¶</a></dt>
<dd><p>Misclassification error rate of QDA and LDA.</p>
<p>Looks like they’re both the same for the Height and Weight data: 11.9%.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.four_exercises.question_19">
<code class="descclassname">chapter.four_exercises.</code><code class="descname">question_19</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/four_exercises.html#question_19"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.four_exercises.question_19" title="Permalink to this definition">¶</a></dt>
<dd><p>Stretched LDA.</p>
<p>This is LDA with the additional parameter that we can multiply the grouped covariance matrix
by a class-specific value to stretch the Gaussians.</p>
<p>This change will cause the resulting model to still be quadratic in <span class="math notranslate nohighlight">\(x\)</span> because we can’t
eliminate all of the squared terms as we did with LDA.</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(y=c|x,\theta) &amp;= \frac{\pi_c|2\pi\Sigma|^{-\frac{1}{2}}\exp(-\frac{1}{2}(x-\mu_c)^T\Sigma^{-1}(x-\mu_c))}{\Sigma_{c'}\pi_{c'}|2\pi\Sigma|^{-\frac{1}{2}}\exp(-\frac{1}{2}(x-\mu_{c'})^T\Sigma^{-1}(x-\mu_{c'}))}\\
    &amp;= \frac{\pi_ck_c^n\exp(-\frac{1}{2}(x-\mu_c)^T\Sigma^{-1}(x-\mu_c))}{\Sigma_{c'}\pi_{c'}k_{c'}^n\exp(-\frac{1}{2}(x-\mu_{c'})^T\Sigma^{-1}(x-\mu_{c'}))}\\
    &amp;= \frac{\pi_ck_c^n\exp(-\frac{1}{2}k_cx^T\Sigma^{-1}x)\exp(-\frac{1}{2}k_c\mu_c^T\Sigma^{-1}\mu_c+k_c\mu_c^T\Sigma^{-1}x}{\Sigma_{c'}\pi_{c'}k_{c'}^n\exp(-\frac{1}{2}k_{c'}x^T\Sigma^{-1}x)\exp(-\frac{1}{2}k_{c'}\mu_{c'}^T\Sigma^{-1}\mu_{c'}+k_{c'}\mu_{c'}^T\Sigma^{-1}x}\\\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.four_exercises.question_22">
<code class="descclassname">chapter.four_exercises.</code><code class="descname">question_22</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/four_exercises.html#question_22"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.four_exercises.question_22" title="Permalink to this definition">¶</a></dt>
<dd><p>QDA with 3 classes.</p>
<p>Side effect: prints the answer that we’re looking for!</p>
<p>Point [-0.5, 0.5] is classified as 1, while [0.5, 0.5] is classified as 2.</p>
<p>Interestingly, they’re both very close to the decision boundary, with the first point
almost in the 3rd class, and the second point almost in the 1st class. The results may
be an artifact due to numerical instability in the code, but visual inspection marks them
as the class labels given above.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.four_exercises.question_23">
<code class="descclassname">chapter.four_exercises.</code><code class="descname">question_23</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/four_exercises.html#question_23"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.four_exercises.question_23" title="Permalink to this definition">¶</a></dt>
<dd><p>Scalar QDA.</p>
<dl class="docutils">
<dt>Side-effect:</dt>
<dd><p class="first">We found the QDA values for each class as desired, and got an 83% probability for a 72 inch
student to be a male at this university.</p>
<p class="last">The meatier question is about how to extend this method to the situation if we had multiple
attributes per person. The clear answer would be to simply run QDA as implemented earlier, but I think
that’s not what the question is really getting at. I think you could run an ensemble of 1D QDA’s and
then take the average of the predicted values for each class across the set of 1D classifiers. This would
ignore correlations between variables, but would take into account the multiple features.</p>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-chapter.one_exercises">
<span id="chapter-one-exercises-module"></span><h2>chapter.one_exercises module<a class="headerlink" href="#module-chapter.one_exercises" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="chapter.one_exercises.FLANN_KNN">
<em class="property">class </em><code class="descclassname">chapter.one_exercises.</code><code class="descname">FLANN_KNN</code><span class="sig-paren">(</span><em>k</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#FLANN_KNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.FLANN_KNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="chapter.one_exercises.FLANN_KNN.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#FLANN_KNN.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.FLANN_KNN.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="chapter.one_exercises.FLANN_KNN.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#FLANN_KNN.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.FLANN_KNN.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="chapter.one_exercises.LinearKNN">
<em class="property">class </em><code class="descclassname">chapter.one_exercises.</code><code class="descname">LinearKNN</code><span class="sig-paren">(</span><em>k</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#LinearKNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.LinearKNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="chapter.one_exercises.LinearKNN.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#LinearKNN.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.LinearKNN.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="chapter.one_exercises.LinearKNN.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>batch_size=2048</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#LinearKNN.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.LinearKNN.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="chapter.one_exercises.download_if_needed">
<code class="descclassname">chapter.one_exercises.</code><code class="descname">download_if_needed</code><span class="sig-paren">(</span><em>url</em>, <em>local</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#download_if_needed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.download_if_needed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.one_exercises.question_1">
<code class="descclassname">chapter.one_exercises.</code><code class="descname">question_1</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#question_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.question_1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.one_exercises.question_2">
<code class="descclassname">chapter.one_exercises.</code><code class="descname">question_2</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#question_2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.question_2" title="Permalink to this definition">¶</a></dt>
<dd><p>This code has some external dependencies. Namely, it uses FLANN and the
python bindings for it. Unfortunately, the maintainer of those bindings
hasn’t fixed some compatibility issues so a pull request needs to be used
to allow it to work with python 3. Or, you can fix the few compatibillity
errors <a class="reference external" href="https://github.com/primetang/pyflann/issues/1">yourself</a> .</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/primetang/pyflann.git
<span class="nb">cd</span> pyflann
git fetch origin pull/7/head:python36
git checkout python36
python setup.py install
</pre></div>
</div>
<p>In the end, the speedup is quite drastic. As the sample size doubles
from 500 to 1000 to 2000 there is almost no change in speed for the FLANN
version; however, the time increases substantially for the Linear version.</p>
</dd></dl>

<dl class="function">
<dt id="chapter.one_exercises.question_3">
<code class="descclassname">chapter.one_exercises.</code><code class="descname">question_3</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#question_3"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.question_3" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.one_exercises.read_mnist_img">
<code class="descclassname">chapter.one_exercises.</code><code class="descname">read_mnist_img</code><span class="sig-paren">(</span><em>local</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#read_mnist_img"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.read_mnist_img" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.one_exercises.read_mnist_labels">
<code class="descclassname">chapter.one_exercises.</code><code class="descname">read_mnist_labels</code><span class="sig-paren">(</span><em>local</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/one_exercises.html#read_mnist_labels"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.one_exercises.read_mnist_labels" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-chapter.six">
<span id="chapter-six-module"></span><h2>chapter.six module<a class="headerlink" href="#module-chapter.six" title="Permalink to this headline">¶</a></h2>
<p>This module covers some tests from chapter 6: Frequentist Methods.</p>
<dl class="class">
<dt id="chapter.six.MajorityClassifier">
<em class="property">class </em><code class="descclassname">chapter.six.</code><code class="descname">MajorityClassifier</code><a class="reference internal" href="_modules/chapter/six.html#MajorityClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.six.MajorityClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Selects the majority label from the training data.</p>
<p>This classifier only works on random data, with a binary label it’s not meant
to be used for actually problems.</p>
<dl class="attribute">
<dt id="chapter.six.MajorityClassifier.prob">
<code class="descname">prob</code><a class="headerlink" href="#chapter.six.MajorityClassifier.prob" title="Permalink to this definition">¶</a></dt>
<dd><p><em>float</em> – the probability of the majority class.</p>
</dd></dl>

<dl class="method">
<dt id="chapter.six.MajorityClassifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>data</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/six.html#MajorityClassifier.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.six.MajorityClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Find out the majority class label from the test labels.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>data</strong> (<em>numpy.array</em>) – an array of labels <span class="math notranslate nohighlight">\(y \in {0, 1}\)</span>.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="chapter.six.MajorityClassifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/six.html#MajorityClassifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.six.MajorityClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the majority class irrespective of input.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">1 or 0.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="chapter.six.ProbabilisticClassifier">
<em class="property">class </em><code class="descclassname">chapter.six.</code><code class="descname">ProbabilisticClassifier</code><span class="sig-paren">(</span><em>seed=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/six.html#ProbabilisticClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.six.ProbabilisticClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Selects a class label by the proportion of labels in the training data.</p>
<dl class="attribute">
<dt id="chapter.six.ProbabilisticClassifier.prob">
<code class="descname">prob</code><a class="headerlink" href="#chapter.six.ProbabilisticClassifier.prob" title="Permalink to this definition">¶</a></dt>
<dd><p><em>float</em> – the probability of getting 1 the data.</p>
</dd></dl>

<dl class="method">
<dt id="chapter.six.ProbabilisticClassifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>data</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/six.html#ProbabilisticClassifier.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.six.ProbabilisticClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Find out the probability of 1’s from the test labels.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>data</strong> (<em>numpy.array</em>) – an array of labels <span class="math notranslate nohighlight">\(y \in {0, 1}\)</span>.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="chapter.six.ProbabilisticClassifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/six.html#ProbabilisticClassifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.six.ProbabilisticClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a stochastic result that is proportional to the number of 1’s in the training data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">1 or 0.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="chapter.six.demo_loocv">
<code class="descclassname">chapter.six.</code><code class="descname">demo_loocv</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/six.html#demo_loocv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.six.demo_loocv" title="Permalink to this definition">¶</a></dt>
<dd><p>Pessimism of LOOCV.</p>
<p>I was thinking about this a little, and I was wondering about this question
beyond what was stated in 6.1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-chapter.six_exercises">
<span id="chapter-six-exercises-module"></span><h2>chapter.six_exercises module<a class="headerlink" href="#module-chapter.six_exercises" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="chapter.six_exercises.question_1">
<code class="descclassname">chapter.six_exercises.</code><code class="descname">question_1</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/six_exercises.html#question_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.six_exercises.question_1" title="Permalink to this definition">¶</a></dt>
<dd><p>Pessimism of LOOCV. I didn’t like the question because I thought it was a little too open…</p>
<p>You want to show that LOOCV can be a really bad estimate of error in some situations. For example, if you
have a random class label that you’re trying to predict (binary, equal proportions), LOOCV can report
a really bad error bound. You’re asked what the best classifier is for this data and what happens when you
do its LOOCV estimate.</p>
<p>The question is leading (though not prescribing) you towards a simple majority classifier. It will be right the
maximum number of times (0.5), and it’s LOOCV error rate will be extremely pessimistic, at 100%. The reason is
simple enough - if you remove a single sample, then the other class becomese the majority so you’ll predict that
but it’s only the majority because the test sample belonged to the other class and so you’ll be wrong every time.</p>
<p>But this isn’t the only optimal classifier specification for this problem. You can also have a probabilistic
classifier that outputs a label in proportion to the input. In this case you’ll again achieve an accuracy of 0.5,
but the LOOCV will essentially also be 0.5 as the training data grows. For small amounts of training data the
process will give you something closer to a 0.49 error rate, but it’s still not that bad.</p>
<p>In short, the LOOCV is pessimistic, but to get the wild swing desired by the question it should have been explicit
and asked the reader to investigate the majority classifier (as is done in the source discussion referred to in the
question (Witten, Frank, Hall p.154 in Data mining 3rd edition).</p>
<p>See <code class="docutils literal notranslate"><span class="pre">chapter.six.demo_loocv</span></code> for simulation study of this question.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-chapter.three_exercises">
<span id="chapter-three-exercises-module"></span><h2>chapter.three_exercises module<a class="headerlink" href="#module-chapter.three_exercises" title="Permalink to this headline">¶</a></h2>
<p>Worked examples and exercises from Chapter 3.</p>
<dl class="class">
<dt id="chapter.three_exercises.BetaBinomial">
<em class="property">class </em><code class="descclassname">chapter.three_exercises.</code><code class="descname">BetaBinomial</code><span class="sig-paren">(</span><em>alpha_0</em>, <em>alpha_1</em>, <em>seed=1337</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#BetaBinomial"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.BetaBinomial" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="chapter.three_exercises.BetaBinomial.posterior">
<code class="descname">posterior</code><span class="sig-paren">(</span><em>samples</em>, <em>freq=100</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#BetaBinomial.posterior"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.BetaBinomial.posterior" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="chapter.three_exercises.BetaBinomial.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#BetaBinomial.sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.BetaBinomial.sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="chapter.three_exercises.Concept">
<em class="property">class </em><code class="descclassname">chapter.three_exercises.</code><code class="descname">Concept</code><span class="sig-paren">(</span><em>name</em>, <em>extension=[]</em>, <em>prior=-1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#Concept"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.Concept" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>This is a concept or hypothesis from the Numbers Game.</p>
<dl class="attribute">
<dt id="chapter.three_exercises.Concept.name">
<code class="descname">name</code><a class="headerlink" href="#chapter.three_exercises.Concept.name" title="Permalink to this definition">¶</a></dt>
<dd><p><em>str</em> – The common name of the concept.</p>
</dd></dl>

<dl class="attribute">
<dt id="chapter.three_exercises.Concept.extension">
<code class="descname">extension</code><a class="headerlink" href="#chapter.three_exercises.Concept.extension" title="Permalink to this definition">¶</a></dt>
<dd><p><em>list int</em> – The values in the event space this concept describes.</p>
</dd></dl>

<dl class="attribute">
<dt id="chapter.three_exercises.Concept.prior">
<code class="descname">prior</code><a class="headerlink" href="#chapter.three_exercises.Concept.prior" title="Permalink to this definition">¶</a></dt>
<dd><p><em>float</em> – The prior probability of this concept being selected.</p>
</dd></dl>

<dl class="method">
<dt id="chapter.three_exercises.Concept.likelihood">
<code class="descname">likelihood</code><span class="sig-paren">(</span><em>n_samples</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#Concept.likelihood"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.Concept.likelihood" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the <span class="math notranslate nohighlight">\(p(D|h) = \frac{p(D,h)}{p(h)}\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>n_samples</strong> (<em>int</em>) – the number of samples collected from the target concept.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><dl class="docutils">
<dt>the likelihood of the data given the hypothesis.</dt>
<dd>This comes from the strong sampling assumption.</dd>
</dl>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list float</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="chapter.three_exercises.NumberGame">
<em class="property">class </em><code class="descclassname">chapter.three_exercises.</code><code class="descname">NumberGame</code><span class="sig-paren">(</span><em>*</em>, <em>seed=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#NumberGame"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.NumberGame" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>The game as described in Chapter 3 but used by Joshua Tenenbaum in his PhD Thesis.</p>
<p>The game demonstrates that it is possible to select a correct hypothesis using only
positive examples from the target.</p>
<p>Note: It is critical that all the candidate concepts are distinct. This might seem obvious,
but there are often many ways to describe the same concept in language. For example, consider
<code class="docutils literal notranslate"><span class="pre">multiples</span> <span class="pre">of</span> <span class="pre">10</span></code> and <code class="docutils literal notranslate"><span class="pre">ends</span> <span class="pre">in</span> <span class="pre">0</span></code> for the space <span class="math notranslate nohighlight">\([1, 100]\)</span>. These are clearly different
ideas, but they have the same extension in the space. This means that if one is the target, the
other is equally likely. This causes the optimization to converge to 0.5 instead of 1.0, playing
havoc with the program.</p>
<dl class="attribute">
<dt id="chapter.three_exercises.NumberGame.concepts">
<code class="descname">concepts</code><a class="headerlink" href="#chapter.three_exercises.NumberGame.concepts" title="Permalink to this definition">¶</a></dt>
<dd><p><em>list Concept</em> – The possible concepts in the game.</p>
</dd></dl>

<dl class="attribute">
<dt id="chapter.three_exercises.NumberGame.active_concept">
<code class="descname">active_concept</code><a class="headerlink" href="#chapter.three_exercises.NumberGame.active_concept" title="Permalink to this definition">¶</a></dt>
<dd><p>(Concept): The current target of the game, selected at random from <code class="docutils literal notranslate"><span class="pre">concepts</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="chapter.three_exercises.NumberGame.plugin_distribution">
<code class="descname">plugin_distribution</code><span class="sig-paren">(</span><em>samples</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#NumberGame.plugin_distribution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.NumberGame.plugin_distribution" title="Permalink to this definition">¶</a></dt>
<dd><p>What is the probability that any point belongs to the target concept
given that we “plug in” the most likely concept a posteriori?</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>samples</strong> – the samples seen so far.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">List of the posterior probabilities using the plug-in estimator.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="chapter.three_exercises.NumberGame.post_predictive_distribution">
<code class="descname">post_predictive_distribution</code><span class="sig-paren">(</span><em>samples</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#NumberGame.post_predictive_distribution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.NumberGame.post_predictive_distribution" title="Permalink to this definition">¶</a></dt>
<dd><p>What is the probability that any point belongs to the
target concept given the data we’ve seen so far?</p>
<div class="math notranslate nohighlight">
\[p(\tilde{x} \in C|\mathcal{D}) = \Sigma_h p(y=1|\tilde{x},h)p(h|\mathcal{D}).\]</div>
<p>Where <span class="math notranslate nohighlight">\(\tilde{x}\)</span> is a future observation and <span class="math notranslate nohighlight">\(y=1\)</span> states that the
observation is consistent with the given concept.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>samples</strong> (<em>list int</em>) – the samples from the target concept we’ve observed so far.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the posterior predictive distribution at this time.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list float</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="chapter.three_exercises.NumberGame.posterior">
<code class="descname">posterior</code><span class="sig-paren">(</span><em>samples</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#NumberGame.posterior"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.NumberGame.posterior" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate the full posterior probability of all <code class="docutils literal notranslate"><span class="pre">concepts</span></code>.</p>
<p>Computes the posterior <span class="math notranslate nohighlight">\(p(C|\mathcal{D}) = \frac{p(\mathcal{D}|h)p(h)}{p(\mathcal{D})}\)</span>,
substituting all possible concepts for <span class="math notranslate nohighlight">\(h\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>(</strong><strong>list</strong> (<em>samples</em>) – int): All samples generated by the target concept so far.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the posterior probability of all <code class="docutils literal notranslate"><span class="pre">concepts</span></code>.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list float</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="chapter.three_exercises.NumberGame.sample_from_concept">
<code class="descname">sample_from_concept</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#NumberGame.sample_from_concept"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.NumberGame.sample_from_concept" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a single sample from the <code class="docutils literal notranslate"><span class="pre">active_concept</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a number from the active concept.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.bb_main">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">bb_main</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#bb_main"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.bb_main" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.likelihood_ratio">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">likelihood_ratio</code><span class="sig-paren">(</span><em>posteriors</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#likelihood_ratio"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.likelihood_ratio" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.numbers_main">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">numbers_main</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#numbers_main"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.numbers_main" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_1">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_1</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_1" title="Permalink to this definition">¶</a></dt>
<dd><p>Optimize the log likelihood of <span class="math notranslate nohighlight">\(p(\mathcal{D}|\theta) = \theta^{N_1}(1-\theta)^{N_0}\)</span>
to prove <span class="math notranslate nohighlight">\(\frac{N_1}{N}\)</span>, the MLE of the Bernoulli/binomial model.</p>
<div class="math notranslate nohighlight">
\[\begin{split}log(p(\mathcal{D}|\theta) &amp;= log(\theta^{N_1}(1-\theta)^{N_0})\\
                          &amp;= N_1 log(\theta)+ N_0 log(1-\theta)\\
                          &amp;= N_1 log(\theta)+ (N-N_1)log(1-\theta).\end{split}\]</div>
<p>Now, optimizing for <span class="math notranslate nohighlight">\(\theta\)</span> by taking the derivative of the above and setting it
equal to 0.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{d}{d\theta} [N_1 log(\theta)+ (N-N_1)log(1-\theta)] &amp;= \frac{N_1}{\theta} - \frac{N-N_1}{1-\theta}\\
0 &amp;= \frac{N_1}{\theta} - \frac{N-N_1}{1-\theta}\\
N_1(1-\theta) &amp;= (N-N_1)\theta\\
N_1 - N_1\theta &amp;= N\theta - N_1\theta\\
N_1 &amp;= N\theta\\
\frac{N_1}{N} &amp;= \theta\\\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_10">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_10</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_10"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_10" title="Permalink to this definition">¶</a></dt>
<dd><p>Taxicab hijinks.</p>
<p>You go to a city and see a taxi numbered 100. Can we figure out how many taxis there are in this city?</p>
<p>a) Assuming we start with a <span class="math notranslate nohighlight">\(Pareto(\theta,0,0)\)</span> distribution on the number, what’s the posterior after
seeing that first taxicab numbered 100?</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(\theta|\mathcal{D}) &amp;= Pareto(\theta|N+K,max(0,100))\\
    &amp;= Pareto(\theta|1,100)\end{split}\]</div>
<ol class="loweralpha" start="2">
<li><p class="first">Compute the posterior mean, mode, and median:</p>
<blockquote>
<div><ol class="lowerroman simple">
<li>mean = DNE, the rate parameter needs to be bigger.</li>
<li>mode = 100, we’ve only seen 1 data point!</li>
<li>median = 200…</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}P(\theta \leq 0.5) &amp;= \\
0.5 &amp;= \int_m^x km^k\theta^{-(k+1)}d\theta\\
    &amp;= 100\int_{100}^x \theta^{-2}d\theta\\
    &amp;= 100\left[-\theta^{-1}\rvert_{100}^x\right]\\
    &amp;= \frac{100}{100} - \frac{100}{x}\\
0.5 &amp;= \frac{100}{x}\\
x = 200.\end{split}\]</div>
</div></blockquote>
</li>
<li><p class="first">Derive an expression for the posterior predictive after <span class="math notranslate nohighlight">\(\mathcal{D}=\{100\}\)</span>.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}p(\mathcal{D'}|\mathcal{D},\alpha) &amp;= \int p(\mathcal{D'}|\theta)p(\theta|\mathcal{D},\alpha)d\theta\\
   &amp;= \int_c^{\infty} Unif(x|\theta)Pareto(\theta|N+K,m)d\theta\\
   &amp;= \int_c^{\infty}\theta^{-1}(N+K)m^{N+K}\theta^{-(N+K+1)}d\theta\\
   &amp;= (N+K)m^{N+K}\int_c^{\infty}\theta^{-(N+K+2)}d\theta\\
   &amp;= (N+K)m^{N+K}\left[\left.\frac{-1}{(N+K+1)\theta^{N+K+1}}\right|_c^{\infty}\right]\\
   &amp;= \frac{m^{N+K}}{c^{N+K+1}}.\end{split}\]</div>
<p>This is all predicated on <span class="math notranslate nohighlight">\(c = max(\mathcal{D'},m)\)</span>. We need to notice that the likelihood of a future
point falling into the existing range doesn’t need to stretch the max, so any future value less than the current
max will get equal probability of happening. If we want to predict the probability of a higher numbered taxi,
then we need to start the integral from that point forward given the evidence we’ve collected so far, so that
should be less likely than a uniform distribution up to that number (since we haven’t seen such a large value
before). Using this formula, we can see that:</p>
<ol class="lowerroman simple">
<li><span class="math notranslate nohighlight">\(p(x=50|\mathcal{D},\alpha) = \frac{1}{100}\)</span></li>
<li><span class="math notranslate nohighlight">\(p(x=100|\mathcal{D},\alpha) = \frac{1}{100}\)</span></li>
<li><span class="math notranslate nohighlight">\(p(x=150|\mathcal{D},\alpha) = \frac{1}{225}\)</span></li>
</ol>
</div></blockquote>
</li>
</ol>
<p>e) There aren’t an infinite number of taxis, so there should be a reasonable upper bound in the integral. The prior
should also be set to a more reasonable value because the 150 case seems unusually low.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_11">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_11</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_11"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_11" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian analysis of the exponential distribution.</p>
<ol class="loweralpha">
<li><p class="first">Derive the MLE of <span class="math notranslate nohighlight">\(Expon(x|\theta)\)</span>.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}p(x|\theta) &amp;= Expon(x|\theta)\\
    &amp;= \theta e^{-x_1\theta}\cdots\theta e^{-x_n\theta}\\
    &amp;= \theta^n e^{-\theta \sum_i x_i},~&amp;\textrm{Take the der. and set to 0}\\
0   &amp;= n\theta^{n-1}e^{-\theta \sum_i x_i} - \theta^n \sum_i x_i e^{-\theta \sum_i x_i}\\
\theta^n \sum_i x_i &amp;= n\theta^{n-1}\\
\frac{\sum_i x_i}{n} &amp;= \frac{1}{\theta}\\
\bar{x} &amp;= \frac{1}{\theta}\\
\frac{1}{\bar{x}} &amp;= \theta.\end{split}\]</div>
</div></blockquote>
</li>
<li><p class="first">Given 3 observations of <span class="math notranslate nohighlight">\(X, {5, 4, 6}\)</span>, what is the MLE of this data? <span class="math notranslate nohighlight">\(\theta = \frac{1}{5}\)</span>.</p>
</li>
</ol>
<p>c) An expert thinks <span class="math notranslate nohighlight">\(p(\theta) = Expon(\theta|\lambda)\)</span>. Choose the prior <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span> such that
<span class="math notranslate nohighlight">\(\mathbb{E}[\theta] = 1/3\)</span>. We can do the MLE route again, to see that <span class="math notranslate nohighlight">\(\theta = \frac{1}{\lambda}\)</span> so
we end up with <span class="math notranslate nohighlight">\(\hat{\lambda} = 3\)</span> to get the desired expected value of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<ol class="loweralpha" start="4">
<li><p class="first">What is the posterior, <span class="math notranslate nohighlight">\(p(\theta|\mathcal{D},\hat{\lambda})\)</span>?</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}p(\theta|\mathcal{D},\hat{\lambda}) &amp;= p(\theta)p(\mathcal{D}|\theta\hat{\lambda})\\
    &amp;\propto Expon(\theta|\hat{\lambda})Expon(x|\theta)\\
    &amp;= \theta e^{-\theta\hat{\lambda}}\theta^{n}e^{-\theta\sum_i x_i}\\
    &amp;= \theta^{n+1}e^{-\theta(\sum_i x_i + \hat{\lambda}}\\
    &amp;= Gamma(\theta|n+2, \sum_i x_i + \hat{\lambda}).\end{split}\]</div>
</div></blockquote>
</li>
</ol>
<p>e) The exponential prior is not conjugate to the exponential likelihood. It results in a Gamma, but it turns out
that the exponential we selected was just a special case of the Gamma distribution. In general, based on this
analysis, I would say that we used a <span class="math notranslate nohighlight">\(Gamma(\theta|2,\hat{\lambda})\)</span> prior and not a <span class="math notranslate nohighlight">\(Expon(\theta|\hat{\lambda})\)</span>
prior since the posterior distribution should be the same form as the prior if it was conjugate.</p>
<ol class="loweralpha simple" start="6">
<li>The posterior mean is <span class="math notranslate nohighlight">\(\frac{n+2}{\sum_i x_i + \hat{\lambda}}\)</span> based on the statistics of the Gamma dist.</li>
</ol>
<p>g) The MLE and the posterior mean differ because there wasn’t a prior involved in the MLE derivation. The prior
suggests that the rate of failure is somewhat shorter than we have observed so far, and this additional information
wasn’t available in just the likelihood alone. The posterior mean is probably more reasonable assuming the experts
can give a reaonable prior estimate from their experience of studying other machines produced by a similar process.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_12">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_12</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_12"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_12" title="Permalink to this definition">¶</a></dt>
<dd><p>Bernoulli MAP estimate with non-conjugate priors.</p>
<ol class="loweralpha">
<li><p class="first">What if you used a prior: <span class="math notranslate nohighlight">\(p(\theta) = 0.5 if \theta = 0.5, 0.5 if \theta = 0.4, 0 otherwise\)</span>.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}p(\theta|D) &amp;= p(\theta)p(D|\theta)\\
    &amp;= p(\theta)\binom{N}{N_1}\theta^{N_1}(1-\theta)^{N-N_1}\\
    &amp;= 0.5\binom{N}{N_1}0.5^{N_1}0.5^{N-N_1}+0.5\binom{N}{N_1}0.4^{N_1}0.6^{N-N_1}\\
    &amp;= 0.5\binom{N}{N_1}\left[0.5^{N}+0.4^{N_1}0.6^{N-N_1}\right].\end{split}\]</div>
</div></blockquote>
</li>
</ol>
<p>b) What if the true parameter is <span class="math notranslate nohighlight">\(\theta = 0.41\)</span>. Which prior works better? For small <span class="math notranslate nohighlight">\(N\)</span>, we’ll likely
find that the new prior is more accurate because it wants a <span class="math notranslate nohighlight">\(\theta\)</span> close to 0.45. Unfortunately, its
effect never really diminishes with increasing trials and so the data doesn’t really overwhelm it. The Beta prior
is more likely to have greater errors in the early stages (unless very specific parameters are selected), but
instead of blending two specific values of <span class="math notranslate nohighlight">\(\theta\)</span> in fixed amounts, the blending is a part of the data
collections and in the limit will converge to the true value.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_2">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_2</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_2" title="Permalink to this definition">¶</a></dt>
<dd><p>Show that:</p>
<div class="math notranslate nohighlight">
\[\frac{[(\alpha_1)\cdots(\alpha_1 + N_1 - 1)][(\alpha_0)\cdots(\alpha_0+N_0-1)]}{(\alpha)\cdots(\alpha+N-1)}\]</div>
<p>Can be reduced to:</p>
<div class="math notranslate nohighlight">
\[\frac{[\Gamma(\alpha_1+N_1)\Gamma(\alpha_0+N_0)]}{\Gamma(\alpha_1+\alpha_0+N)}\frac{\Gamma(\alpha_1+\alpha_0)}{\Gamma(\alpha_1)\Gamma(\alpha_0)}\]</div>
<p>Using <span class="math notranslate nohighlight">\((\alpha-1)! = \Gamma(\alpha)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{[(\alpha_1)\cdots(\alpha_1 + N_1 - 1)][(\alpha_0)\cdots(\alpha_0+N_0-1)]}{(\alpha)\cdots(\alpha+N-1)} &amp;=\\
\frac{[(\alpha_1)\cdots(\alpha_1 + N_1 - 1)][(\alpha_0)\cdots(\alpha_0+N_0-1)]}{(\alpha)\cdots(\alpha+N-1)}\cdot\frac{(\alpha-1)!}{(\alpha-1)!} &amp;=\\
\frac{[(\alpha_1)\cdots(\alpha_1 + N_1 - 1)][(\alpha_0)\cdots(\alpha_0+N_0-1)]}{(\alpha+N-1)!}\cdot\frac{(\alpha-1)!}{1} &amp;=~,~&amp;\textrm{Def. of factorial}\\
\frac{[(\alpha_1)\cdots(\alpha_1 + N_1 - 1)][(\alpha_0)\cdots(\alpha_0+N_0-1)]}{(\alpha+N-1)!}\cdot\frac{(\alpha-1)!(\alpha_1-1)!(\alpha_0-1)!}{(\alpha_1-1)!(\alpha_0-1)!} &amp;=\\
\frac{(\alpha_1 + N_1 - 1)!(\alpha_0+N_0-1)!}{(\alpha+N-1)!}\cdot\frac{(\alpha-1)!}{(\alpha_1-1)!(\alpha_0-1)!} &amp;=~,~&amp;\textrm{Def. of factorial}\\
\frac{\Gamma(\alpha_1 + N_1)\Gamma(\alpha_0+N_0)}{\Gamma(\alpha+N)}\cdot\frac{\Gamma(\alpha)}{\Gamma(\alpha_1)\Gamma(\alpha_0)} &amp;=~,~&amp;\textrm{By the given}\\
\frac{\Gamma(\alpha_1 + N_1)\Gamma(\alpha_0+N_0)}{\Gamma(\alpha_1+\alpha_0+N)}\cdot\frac{\Gamma(\alpha_1+\alpha_0)}{\Gamma(\alpha_1)\Gamma(\alpha_0)} &amp;=~,~&amp;\textrm{Def.}~\alpha = \alpha_1+\alpha_0\end{split}\]</div>
<p>So we see that even without appealing to the Beta distribution, we can by sheer counts of the probability of the
data occurring arrive at the marginal likelihood for the Beta-Bernoulli model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_3">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_3</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_3"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_3" title="Permalink to this definition">¶</a></dt>
<dd><p>Posterior predictive for Beta-Binomial model</p>
<p>Prove that <span class="math notranslate nohighlight">\(p(x|n, \mathcal{D})=\frac{B(x+\alpha_1',n-x+\alpha_0')}{B(\alpha_1',\alpha_0')}\binom{n}{x}\)</span>
reduces to <span class="math notranslate nohighlight">\(p(\tilde{x}=1|\mathcal{D})=\frac{\alpha_1'}{\alpha_1'+\alpha_0'}\)</span> when <span class="math notranslate nohighlight">\(n=1\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{B(x+\alpha_1',n-x+\alpha_0')}{B(\alpha_1',\alpha_0')}\binom{n}{x} &amp;=\\
\frac{B(x+\alpha_1',1-x+\alpha_0')}{B(\alpha_1',\alpha_0')}\binom{1}{x} &amp;=~&amp;,~\textrm{Given}\\
\frac{B(1+\alpha_1',1-1+\alpha_0')}{B(\alpha_1',\alpha_0')}\cdot 1 &amp;=~&amp;,~\textrm{Given}~x=1\\
\frac{\Gamma(1+\alpha_1')\Gamma(\alpha_0')\Gamma(\alpha_1'+\alpha_0')}{\Gamma(1+\alpha_1'+\alpha_0')\Gamma(\alpha_1')\Gamma(\alpha_0')} &amp;=~&amp;,~\textrm{Def. of}~Beta\\
\frac{\alpha_1'\Gamma(\alpha_1')\Gamma(\alpha_1'+\alpha_0')}{(\alpha_1'+\alpha_0')\Gamma(\alpha_1'+\alpha_0')\Gamma(\alpha_1')} &amp;=~&amp;,~\Gamma(a+1)=a\Gamma(a)\\
\frac{\alpha_1'}{\alpha_1'+\alpha_0'}.\end{split}\]</div>
<p>So we can see that after a single trial, the posterior predictive of getting a 1 in that trial is simply the rate
of getting a 1 as given by the prior, which makes sense because we haven’t yet observed any data.
:returns: None.</p>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_4">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_4</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_4"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_4" title="Permalink to this definition">¶</a></dt>
<dd><p>Simple mixture distribution.</p>
<p>Let’s say we tossed a fair coin 5 times and know that &lt; 3 heads appeared. Compute the posterior up to
normalization constant..</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(X &lt; 3 | \theta) &amp;= p(X=0 | \theta) + p(X=1 | \theta) + p(X=2|\theta),~&amp;\textrm{Union of mutually exclusive events.}\\
                  &amp;\propto B(\theta|1,1)Bin(0|\theta,5) + B(\theta|1,1)Bin(1|\theta,5) + B(\theta|1,1)Bin(2|\theta,5),~&amp;\textrm{Bayes law}\\
                  &amp;\propto B(\theta|1,6) + B(\theta|2,5) + B(\theta|3,4),~&amp;\textrm{Conjugate prior}.\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_5">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_5</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_5"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_5" title="Permalink to this definition">¶</a></dt>
<dd><p>Uninformative prior for log-odds ratio.</p>
<p>Let <span class="math notranslate nohighlight">\(\phi = \textrm{logit}(\theta) = log\frac{\theta}{1-\theta}\)</span>.
Show that if <span class="math notranslate nohighlight">\(p(\phi) \propto 1\)</span>, then <span class="math notranslate nohighlight">\(p(\theta) \propto Beta(\theta|0, 0)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(\phi) &amp;= p(\theta)\left\vert\frac{d \theta}{d \phi}\right\vert,~&amp;\textrm{Change of variables}\\
        &amp;= log\frac{\theta}{1-\theta}\left\vert\frac{d\theta}{d\phi}\right\vert,\\
        &amp;= log\theta - log(1-\theta)\left\vert\frac{d\theta}{d\phi}\right\vert,\\
        &amp;= \frac{1}{\theta} + \frac{1}{1-\theta},\\
        &amp;= \frac{\theta + 1 - \theta}{\theta(1-\theta)},\\
        &amp;= \frac{1}{\theta(1-\theta)},\\
        &amp;= \theta^{-1}(1-\theta)^{-1},\\
        &amp;= B(\theta|0, 0),~&amp;\textrm{Def. of Beta}.\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_6">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_6</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_6"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_6" title="Permalink to this definition">¶</a></dt>
<dd><p>MLE for the Poisson distribution.
<span class="math notranslate nohighlight">\(Poi(x|\lambda) = e^{-\lambda}\frac{\lambda^x}{x!}\)</span>. Derive the MLE.</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(\lambda|x_1,\ldots,x_n) &amp;= e^{-\lambda}\frac{\lambda^{x_1}}{x_1!}\cdots e^{-\lambda}\frac{\lambda^{x_n}}{x_n!},~&amp;X~\sim~Poi(\lambda)\\
    &amp;= e^{-n\lambda}\frac{\lambda^{x_1+\cdots+x_n}}{\prod_i^n x_i!}.\end{split}\]</div>
<p>Set take the derivative and set it equal to 0 to find the maximum:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{d}{d\lambda}p(\lambda|x_1,\ldots,x_n) &amp;= -ne^{-n\lambda}\frac{\lambda^{x_1+\cdots+x_n}}{\prod_i^n x_i!} + e^{-n\lambda}\frac{(x_1+\cdots+x_n)\lambda^{x_1+\cdots+x_n-1}}{\prod_i^n x_i!},\\
    ne^{-n\lambda}\frac{\lambda^{x_1+\cdots+x_n}}{\prod_i^n x_i!} &amp;= e^{-n\lambda}\frac{(x_1+\cdots+x_n)\lambda^{x_1+\cdots+x_n-1}}{\prod_i^n x_i!},\\
    n\lambda^{x_1+\cdots+x_n} &amp;= (x_1+\cdots+x_n)\lambda^{x_1+\cdots+x_n-1},\\
    n\lambda &amp;= \sum_i^n x_i,\\
    \lambda &amp;= \frac{1}{n}\sum_i^n x_i.\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_7">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_7</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_7"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_7" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian derivation of Poisson MLE.</p>
<ol class="loweralpha simple">
<li>Derive the posterior assuming a conjugate prior <span class="math notranslate nohighlight">\(p(\lambda) = Ga(\lambda|a,b) \propto \lambda^{a-1}e^{-\lambda b}\)</span>.</li>
</ol>
<p>From the above, the likelihood of the Poisson distribution is: <span class="math notranslate nohighlight">\(e^{-n\lambda}\frac{\lambda^{x_1+\cdots+x_n}}{\prod_i^n x_i!}\)</span>,
so we can write the posterior as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(\lambda|D) &amp;\propto \lambda^{a-1}e^{-\lambda b}e^{-n\lambda}\lambda^{x_1+\cdots+x_n}\\
    &amp;= e^{-\lambda b -\lambda n}\lambda^{a+x_1+\cdots+x_n-1}\\
    &amp;= Ga(a+x_1+\cdots+x_n-1, b+n)\end{split}\]</div>
<ol class="loweralpha simple" start="2">
<li>The MLE of the posterior looks can be found:</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}0 &amp;= \frac{d}{d\lambda}e^{-\lambda b -\lambda n}\lambda^{a+x_1+\cdots+x_n-1}\\
  &amp;= -(b+n)e^{-\lambda(b+n)}\lambda^{a+x_1+\cdots+x_n-1}+e^{-\lambda b -\lambda n}(a+x_1+\cdots+x_n-1)\lambda^{a+x_1+\cdots+x_n-2}\\
(b+n)e^{-\lambda(b+n)}\lambda^{a+x_1+\cdots+x_n-1} &amp;= e^{-\lambda b -\lambda n}(a+x_1+\cdots+x_n-1)\lambda^{a+x_1+\cdots+x_n-2}\\
(b+n)\lambda^{a+x_1+\cdots+x_n-1} &amp;= (a+x_1+\cdots+x_n-1)\lambda^{a+x_1+\cdots+x_n-2}\\
(b+n)\lambda &amp;= a+x_1+\cdots+x_n-1\\
\lambda &amp;= \frac{a-1+\sum_i  x_i}{b+n}.\end{split}\]</div>
<p>If we look at what happens as the prior parameters <span class="math notranslate nohighlight">\(a, b \rightarrow 0\)</span>, we see that the mean of the posterior
approaches the MLE of the Poisson distribution.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_8">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_8</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_8"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_8" title="Permalink to this definition">¶</a></dt>
<dd><p>MLE for the uniform distribution.</p>
<ol class="loweralpha simple">
<li>What is the MLE for data <span class="math notranslate nohighlight">\(x_1,\ldots,x_n\)</span>?</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}p(a|x_1,\ldots,x_n) &amp;= \frac{\sum_i x_i}{(2a)^2}&amp;\\
    &amp;= \frac{-\sum_i x_i}{8a},~&amp;\textrm{Der. with respect to}~a\\
0   &amp;= \frac{-\sum_i x_i}{8a},~&amp;\textrm{Set equal to 0 to find maximum}.\end{split}\]</div>
<p>This is where some insight comes in. Solving for <span class="math notranslate nohighlight">\(a\)</span> doesn’t really work, but
if you think about plotting this function (for some fixed sum of data), you see that
it approaches 0 as <span class="math notranslate nohighlight">\(a\)</span> increases. The MLE occurs when <span class="math notranslate nohighlight">\(\hat{a} = max |x_i|, x_i \in {x_1,\ldots,x_n}\)</span>
since it captures all the data seen so far and there’s no support for anything further out
in magnitude on the number line.</p>
<ol class="loweralpha simple" start="2">
<li>The probability the model would assign to point <span class="math notranslate nohighlight">\(x_{n+1}\)</span> is:</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}p(x_{n+1}|\hat{a}) &amp;= \frac{1}{2\hat{a}},\\
    &amp;= \frac{1}{2x_{\textrm{max}}}.\end{split}\]</div>
<ol class="loweralpha simple" start="3">
<li>This doesn’t make a great deal of sense, especially if only a few data points have been observed.</li>
</ol>
<p>It states that
the only points that will be observed will be no bigger than <span class="math notranslate nohighlight">\(|x_{\textrm{max}}|\)</span>. In reality, the only thing
we know for sure is that <span class="math notranslate nohighlight">\(a\)</span> is at least that large. We may instead set a prior on <span class="math notranslate nohighlight">\(a\)</span> that takes into
account the range of feasible values based on the specific problem. It would have the effect of broadening the range
of values we expect to see, eventually tightening to all the points we’ve seen so far.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.three_exercises.question_9">
<code class="descclassname">chapter.three_exercises.</code><code class="descname">question_9</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/three_exercises.html#question_9"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.three_exercises.question_9" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian analysis of the uniform dist.</p>
<p>Given a Pareto prior, the joint distribution of <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is
<span class="math notranslate nohighlight">\(p(\mathcal{D}, \theta) = \frac{Kb^K}{\theta^{N+K+1}}\mathbb{I}(\theta \geq max(\mathcal{D},b))\)</span>. We’re also
given <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span>, and are asked to derive the posterior <span class="math notranslate nohighlight">\(p(\theta|\mathcal{D})\)</span>. So…</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(\theta|\mathcal{D}) &amp;= \frac{p(\theta)p(\mathcal{D}|\theta)}{p(\mathcal{D})},~&amp;\textrm{Bayes rule}\\
    &amp;= \frac{p(\theta)p(\mathcal{D},\theta)}{p(\theta)p(\mathcal{D})},~&amp;\textrm{Def. of Cond Prob.}\\
    &amp;= \frac{p(\mathcal{D},\theta)}{p(\mathcal{D})}\\
    &amp;= \frac{Kb^K}{\theta^{N+K+1}}\cdot\frac{(N+K)m^{N+K}}{Kb^K},~&amp;\textrm{If max is }\geq b,~\textrm{or}\\
    &amp;= \frac{(N+K)m^{N+K}}{\theta^{N+K+1}},\\
    &amp;= \frac{Kb^K}{\theta^{N+K+1}}\cdot\frac{(N+K)b^{N}}{K},~&amp;\textrm{If max is }&lt; b,\\
    &amp;= \frac{(N+K)b^{N+K}}{\theta^{N+K+1}},\\
    &amp;= Pareto(\theta|N+K, max(\mathcal{D},b)),~&amp;\textrm{Def. of the Pareto distribution}.\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-chapter.two_exercises">
<span id="chapter-two-exercises-module"></span><h2>chapter.two_exercises module<a class="headerlink" href="#module-chapter.two_exercises" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="chapter.two_exercises.question_1">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_1</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_1" title="Permalink to this definition">¶</a></dt>
<dd><p>p(gender=boy) = 0.5
p(gender=girl) = 0.5</p>
<p>Possible outcomes of 2 children:
boy, girl
boy, boy
girl, boy
girl, girl</p>
<p>a) If you know the neighbor has at least one boy, what is the probability the neighbor has a girl?
Sample space: (b,g), (b,b), (g,b). 2/3 events have a girl involved, and they all have equal probability so 2/3.</p>
<p>b) What is the probability that the other child is a girl if you see that one is a boy?
Sample space: (b,g), (b,b). 1/2. The children are independent of each other, so it’s the same as the probability
of one child being a girl.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_10">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_10</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_10"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_10" title="Permalink to this definition">¶</a></dt>
<dd><p>Derive the inverse gamma distribution.</p>
<p>If <span class="math notranslate nohighlight">\(X \sim Ga(a, b)\)</span>, and <span class="math notranslate nohighlight">\(Y = 1/X\)</span>, show that <span class="math notranslate nohighlight">\(Y \sim IG(a, b)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}p_y(y) &amp;= p_x(x)\left|\frac{dy}{dx}\frac{1}{X}\right|\\
    &amp;= \frac{b^a}{\Gamma(a)}\left(\frac{1}{x}\right)^{a-1}e^{-b/x}x^{-2}\\
    &amp;= \frac{b^a}{\Gamma(a)}x^{-(a-1)}e^{-b/x}x^{-2}\\
    &amp;= \frac{b^a}{\Gamma(a)}x^{-(a+1)}e^{-b/x}\\
    &amp;= IG(a, b).\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_11">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_11</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_11"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_11" title="Permalink to this definition">¶</a></dt>
<dd><p>Derive the 1D Gaussian normalization constant.</p>
<p>We’re going to need to do a little u-substitution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}u &amp;= \frac{r^2}{2\sigma^2}\\
du &amp;= \frac{2r}{2\sigma^2}dr\\
\frac{\sigma^2}{r}du &amp;= dr.\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}Z^2 &amp;= \int_0^{2\pi}\int_0^{\infty}r exp\left(\frac{-r^2}{2\sigma^2}\right) dr d\theta\\
    &amp;= \int_0^{2\pi}\int_0^{\infty}r exp\left(\frac{-r^2}{2\sigma^2}\right) dr d\theta\\
    &amp;= \int_0^{2\pi}\int_0^{\infty}re^{-u} du\frac{\sigma^2}{r}d\theta\\
    &amp;= \sigma^2\int_0^{2\pi}\int_0^{\infty}e^{-u} du d\theta\\
    &amp;= \sigma^2\int_0^{2\pi} \left.-e^{-u}\right|_0^{\infty} d\theta\\
    &amp;= \sigma^2\int_0^{2\pi} 1 d\theta\\
    &amp;= \sigma^2\left.\theta\right|_0^{2\pi}\\
    &amp;= \sigma^2 2\pi\\
Z &amp;= \sqrt{\sigma^2 2\pi}\\\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_12">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_12</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_12"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_12" title="Permalink to this definition">¶</a></dt>
<dd><p>Express I(X,Y) as entropy…</p>
<div class="math notranslate nohighlight">
\[\begin{split}I(X,Y) &amp;= \Sigma_x\Sigma_y p(x,y) \log\frac{p(x,y)}{p(x)p(y)}\\
    &amp;= \Sigma_x\Sigma_y p(x|y)p(y) \log\frac{p(x|y)p(y)}{p(x)p(y)}\\
    &amp;= \Sigma_x\Sigma_y p(x|y)p(y) \left[\log p(x|y) - \log p(x)\right]\\
    &amp;= \Sigma_x\Sigma_y p(x|y)p(y)\log p(x|y) - \Sigma_x\Sigma_y p(x|y)p(y)\log p(x)\\
    &amp;= \Sigma_y p(y) \Sigma_x p(x|y)\log p(x|y) - \Sigma_x \log p(x) \Sigma_y p(x|y)p(y)\\
    &amp;= -H(X|Y) - \Sigma_x \log p(x) \Sigma_y p(x|y)p(y),~&amp;\textrm{Def. of Cond. Entropy}\\
    &amp;= -H(X|Y) - \Sigma_x p(x)\log p(x),~&amp;\textrm{Law of Total Prob.}\\
    &amp;= -H(X|Y) + H(X),~&amp;\textrm{Def. of Cond. Entropy}\\
    &amp;= H(X) - H(X|Y).\end{split}\]</div>
<p>You could simply change the way you go from joint to conditional variables in first step of the proof.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_13">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_13</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_13"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_13" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_14">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_14</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_14"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_14" title="Permalink to this definition">¶</a></dt>
<dd><p>Show that normalized mutual information is a type of correlation.</p>
<p><span class="math notranslate nohighlight">\(r = 1-\frac{H(Y|X)}{H(X)}\)</span>.</p>
<ol class="loweralpha">
<li><p class="first">Show <span class="math notranslate nohighlight">\(r = \frac{I(X,Y)}{H(X)}\)</span>:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}r &amp;= 1 - \frac{H(Y|X)}{H(X)}\\
    &amp;= \frac{H(X)}{H(X)} - \frac{H(Y|X)}{H(X)}\\
    &amp;= \frac{H(Y) - H(Y|X)}{H(X)},~&amp;\textrm{X and Y are identically distributed.}\\
    &amp;= \frac{I(X,Y)}{H(X)},~&amp;\textrm{From Q2.12}.\end{split}\]</div>
</div></blockquote>
</li>
</ol>
<p>b) Show <span class="math notranslate nohighlight">\(0 \leq r \leq 1\)</span>. We need to minimize and maximize the numerator. It is minimized when
the <span class="math notranslate nohighlight">\(log\frac{p(x,y)}{p(x)p(y)}\)</span> is minimized, so:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}0 &amp;= \log\frac{p(x,y)}{p(x)p(y)}\\
\log(p(x)p(y)) &amp;= \log p(x,y)\\
\log(p(x)p(y)) &amp;= \log(p(x)p(y)),~&amp;X \perp Y.\end{split}\]</div>
<p>If this term is 0 (and it can be if <span class="math notranslate nohighlight">\(X \perp Y\)</span>), then the numerator is 0 and <span class="math notranslate nohighlight">\(r=0\)</span>. The
numerator is maximized when <span class="math notranslate nohighlight">\(X=Y\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}I(X,X) &amp;= \Sigma_x \Sigma_y p(x,x) \log\frac{p(x,x)}{p(x)p(x)}\\
    &amp;= \Sigma_x \Sigma_y p(x) \log\frac{1}{p(x)}\\
    &amp;= \Sigma_x p(x) \log\frac{1}{p(x)}\\
    &amp;= \Sigma_x p(x) \log 1 - \Sigma_x p(x) \log p(x)\\
    &amp;= 0 + H(X)\\.\end{split}\]</div>
<p>So we end up with <span class="math notranslate nohighlight">\(\frac{H(X)}{H(X)} = 1\)</span>. So we’ve seen the min and max and we have shown that
<span class="math notranslate nohighlight">\(0 \leq r \leq 1\)</span>.</p>
</div></blockquote>
<ol class="loweralpha simple" start="3">
<li><span class="math notranslate nohighlight">\(r = 0\)</span> when <span class="math notranslate nohighlight">\(X \perp Y\)</span>.</li>
<li><span class="math notranslate nohighlight">\(r = 1\)</span> when <span class="math notranslate nohighlight">\(X = Y\)</span>.</li>
</ol>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_16">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_16</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_16"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_16" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean, median, and mode of the Beta distribution.</p>
<ol class="loweralpha">
<li><p class="first">Mean:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}[X] &amp;= \int_0^1 x B(a,b)dx\\
    &amp;= \int_0^1 x\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}dx\\
    &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x^a(1-x)^{b-1}dx\\
    &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x^{(a+1)-1}-x^{a+b-1}dx\\
    &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}B(a+1,b),~&amp;\textrm{Integral form of Beta}\\
    &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+b+1)},~&amp;\textrm{Def. of Beta}\\
    &amp;= \frac{\Gamma(a+b)}{\Gamma(a)}\frac{a\Gamma(a)}{(a+b)\Gamma(a+b)},~&amp;\textrm{Def. of }\Gamma,\\
    &amp;= \frac{a}{(a+b)}.\end{split}\]</div>
</div></blockquote>
</li>
<li><dl class="first docutils">
<dt>Mode:</dt>
<dd><p class="first">We’re going to take the derivative, set to 0, and solve to see what we get…</p>
<div class="last math notranslate nohighlight">
\[\begin{split}B(a, b) &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}dx\\
0 &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left[(a-1)x^{a-2}(1-x)^{b-1}-x^{a-1}(b-1)(1-x)^{b-2}\right]\\
0 &amp;= (a-1)(1-x)-x(b-1)\\
0 &amp;= a-x-1-ax-xb+x\\
ax+bx-2x &amp;= a-1\\
x &amp;= \frac{a-1}{a+b-2}.\end{split}\]</div>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Variance:</dt>
<dd><p class="first">Just going to use the standard formula and hope for the best!</p>
<div class="last math notranslate nohighlight">
\[\begin{split}Var(B(a,b)) &amp;= \int_0^1\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left(x-\frac{a}{a+b}\right)^2 x^{a-1}(1-x)^{b-1}dx\\
    &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1\left(x^2-\frac{2xa}{a+b}+\frac{a^2}{(a+b)^2}\right) x^{a-1}(1-x)^{b-1}dx\\
    &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left[\int_0^1 x^{(a+2)-1}(1-x)^{b-1}dx -\frac{2a}{a+b}\int_0^1 x^{(a+1)-1}(1-x)^{b-1}dx+\frac{a^2}{(a+b)^2}\int_0^1 x^{a-1}(1-x)^{b-1}dx\right]\\
    &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left[B(a+2,b) -\frac{2a}{a+b}B(a+1,b)+\frac{a^2}{(a+b)^2}B(a,b)\right]\\
    &amp;= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left[\frac{\Gamma(a+2)\Gamma(b)}{\Gamma(a+b+2)} -\frac{2a}{a+b}\frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+b+1)}+\frac{a^2}{(a+b)^2}\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\right]\\
    &amp;= \frac{\Gamma(a+b)}{\Gamma(a)}\left[\frac{(a+1)a\Gamma(a)}{(a+b+1)(a+b)\Gamma(a+b)} -\frac{2a}{a+b}\frac{a\Gamma(a)}{(a+b)\Gamma(a+b)}+\frac{a^2}{(a+b)^2}\frac{\Gamma(a)}{\Gamma(a+b)}\right]\\
    &amp;= \frac{(a+1)a}{(a+b+1)(a+b)} -\frac{2a}{a+b}\frac{a}{(a+b)}+\frac{a^2}{(a+b)^2}\\
    &amp;= \frac{a^2+a}{(a+b+1)(a+b)} -\frac{2a^2}{(a+b)^2}+\frac{a^2}{(a+b)^2}\\
    &amp;= \frac{(a^2+a)(a+b) - (2a^2)(a+b+1) + a^2(a+b+1)}{(a+b+1)(a+b)^2}\\
    &amp;= \frac{a^3+a^2b+a^2+ab - 2a^3-2a^2b-2a^2 + a^3+a^2b+a^2}{(a+b+1)(a+b)^2}\\
    &amp;= \frac{ab}{(a+b+1)(a+b)^2}.\end{split}\]</div>
</dd>
</dl>
</li>
</ol>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_17">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_17</code><span class="sig-paren">(</span><em>k=2</em>, <em>trials=1000</em>, <em>seed=1337</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_17"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_17" title="Permalink to this definition">¶</a></dt>
<dd><p>Expected value of the minimum of 2 uniformly distributed numbers…</p>
<p>The trick here is figuring out how to express the max function over two variables…
Assuming <span class="math notranslate nohighlight">\(x_1,x_2 \sim Unif(0,1)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}[min(x_1,x_2)] &amp;= \int_0^1\int_0^1 x_2\mathbb{I}(x_2 \leq x_1) + x_1\mathbb{I}(x_1 &lt; x_2)dx_2 dx_1\\
    &amp;= \int_0^1\int_0^1 x_2\mathbb{I}(x_2 \leq x_1) dx_2 dx_1 + \int_0^1\int_0^1 x_1\mathbb{I}(x_1 &lt; x_2)dx_2 dx_1\\
    &amp;= \int_0^1\int_0^{x_1} x_2 dx_2 dx_1 + \int_0^1\int_0^{x_2} x_1dx_1 dx_2\\
    &amp;= 2\int_0^1\int_0^{x_1} x_2 dx_2 dx_1\\
    &amp;= 2 \frac{1}{2\cdot 3}\\
    &amp;= \frac{1}{3}.\end{split}\]</div>
<p>In general, if you have <span class="math notranslate nohighlight">\(n\)</span> variables from this distribution you can find the expected value of the min
as <span class="math notranslate nohighlight">\(n!\int_0^1\cdot\int_0^{x_n}x_n dx_n\cdots dx_1 = \frac{1}{n+1}\)</span> if you’re talking about the uniform.</p>
<p>Also, as part of experimenting to get this solution, I also did a categorical case, which is very similar except
you need to worry about the situation where the two variables are equal (which has 0 probability in the continuous
case): <span class="math notranslate nohighlight">\(\frac{2}{n^2}\sum_{i=1}^n\sum_{j=1}^i x_j - \sum_{i=1}^n x_i\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of elements
in the space and they are in ascending order. I believe going from 2 to <span class="math notranslate nohighlight">\(k\)</span>
draws will be similar, replacing the numerator with <span class="math notranslate nohighlight">\(k!\)</span> and the denominator with <span class="math notranslate nohighlight">\(k\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>k</strong> (<em>int</em>) – Number of draws from the distribution to compute the min over.</li>
<li><strong>trials</strong> (<em>int</em>) – Number of random min samples to select before computing the expected value.</li>
<li><strong>seed</strong> (<em>int</em>) – Random seed for reproducibility.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">the expected value of the min of <code class="docutils literal notranslate"><span class="pre">k</span></code> uniformly distributed variables.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">float</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_2">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_2</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_2" title="Permalink to this definition">¶</a></dt>
<dd><p>There is a blood found at a crime scene that has no innocent explanation. The blood is of a type found in
1% of the population.</p>
<p>a) Prosecutor’s fallacy: 1% chance that the defendant would have the crime scene blood type if he was innocent,
therefore there is a 99% chance that he is guilty.</p>
<p>This is not what the evidence states: 1% of the population could have committed the crime because only they have
the suspect blood type. The defendant has that blood type, so he is 1/K people who are in consideration for
committing the crime, not 99% likely to have committed the crime. 99% of the population is not in consideration
for the crime at all, but based on the blood evidence alone we cannot state the likelihood of this single
defendent having committed this crime, only that he is in the consideration set.</p>
<p>b) Defendant’s fallacy: There are 800K people in the city, 8000 have the blood type in question. There is just
1 in 8000 chance that the defendant is guilty and so has no relevance.</p>
<p>While it is true that the defendant is just 1 of 8000 city dwellers that have the matching blood type, the blood
is relevant. The true culprit must have that blood type, and so it establishes that further evidence must be
produced to establish the innocence or guilt of the defendant. This is far from the situation that we can ignore
the blood type, the guilty part(ies) must have that match to be considered for the crime.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_3">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_3</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_3"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_3" title="Permalink to this definition">¶</a></dt>
<dd><p>Variance of a sum.</p>
<div class="math notranslate nohighlight">
\[\begin{split}cov[X, Y] &amp;= \mathbb{E}[[X - \mathbb{E}[X]][Y - \mathbb{E}[Y]]]\\
    &amp;= \mathbb{E}[XY - X\mathbb{E}[Y] - Y\mathbb{E}[X] + \mathbb{E}[X]\mathbb{E}[Y]]\\
    &amp;= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[X]\mathbb{E}[Y] + \mathbb{E}[X]\mathbb{E}[Y]\\
    &amp;= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}var[X + Y] &amp;= \mathbb{E}[(X + Y - \mathbb{E}[X+Y])^2]\\
    &amp;= \mathbb{E}[X^2] + \mathbb{E}[XY] - \mathbb{E}[X\mathbb{E}[X+Y]] + \mathbb{E}[XY] + \mathbb{E}[Y^2] - \mathbb{E}[Y\mathbb{E}[X+Y]] - \mathbb{E}[X\mathbb{E}[X+Y]] - \mathbb{E}[Y\mathbb{E}[X+Y]] + \mathbb{E}[X+Y]^2\\
    &amp;= \mathbb{E}[X^2] - \mathbb{E}[X]^2 - \mathbb{E}[X]\mathbb{E}[Y] + \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 - \mathbb{E}[X]\mathbb{E}[Y] +2\mathbb{E}[XY] - \mathbb{E}[X]^2 - 2\mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[Y]^2 + \mathbb{E}[X+Y]^2\\
    &amp;= var(X) + var(Y) + 2\mathbb{E}[XY] - 4\mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[X]^2 - \mathbb{E}[Y]^2 + \mathbb{E}[X+Y]^2\\
    &amp;= var(X) + var(Y) + 2cov(X, Y) - 2\mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[X]^2 - \mathbb{E}[Y]^2 + \mathbb{E}[X]^2 + 2\mathbb{E}[X]\mathbb{E}[Y] + \mathbb{E}[Y]^2\\
    &amp;= var(X) + var(Y) + 2cov(X, Y)\\\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_4">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_4</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_4"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_4" title="Permalink to this definition">¶</a></dt>
<dd><p>Given:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(T=p|D=p) &amp;= 0.99\\
P(T=n|D=n) &amp;= 0.99\\
P(D=p) &amp;= 1/10,000\end{split}\]</div>
<p>This is an application of Bayes Theorem since we want to update the prior probability of having
the disease after knowing the test came back positive. So we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(D=p|T=p) &amp;= \frac{P(T=p|D=p) \cdot P(D=p)}{P(T=p)}, &amp;~\textrm{Bayes Thm.}\\
           &amp;= \frac{P(T=p|D=p) \cdot P(D=p)}{\Sigma_d P(T=p|D=d)\cdot P(D=d)}, &amp;~\textrm{Law of Total Prob.}\\
           &amp;= \frac{P(T=p|D=p) \cdot P(D=p)}{P(T=p|D=p) \cdot P(D=p) + P(T=p|D=n) \cdot P(D=n)}, &amp;~\textrm{Notation}\\
           &amp;= \frac{0.99 \cdot 0.0001}{0.99 \cdot 0.0001 + 0.01 \cdot 0.9999}, &amp;~\textrm{Law of Total Prob.}\\
           &amp;\approx 0.0098.\end{split}\]</div>
<p>This means that the good news is the probability of having the disease is still a little less than 1/100. Also,
The second application of the Law of Total Probability is actually two applications:</p>
<div class="math notranslate nohighlight">
\[\begin{split}1 &amp;= P(D=p) + P(D=n)\\
1 &amp;= P(T=p|D=p) + P(T=p|D=n)\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_5">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_5</code><span class="sig-paren">(</span><em>num_samples=1000000</em>, <em>seed=1337</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_5"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_5" title="Permalink to this definition">¶</a></dt>
<dd><p>The Monty Hall Problem using Bayes theorem.</p>
<p>We’re interested in determining whether switching doors is better than sticking with the original.</p>
<p>Let <span class="math notranslate nohighlight">\(C \sim Unif(3)\)</span> be the random variable representing where the car (prize) is,
<span class="math notranslate nohighlight">\(F \sim Unif(3)\)</span> be the random variable
representing the first selection made by the contestant, and <span class="math notranslate nohighlight">\(O\)</span> be the random variable representing
which door is opened after the first selection is made. This variable is deterministic when the first guess does
not equal the prize value but has a choice otherwise.</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(F=P|O, P) &amp;= \frac{P(O|F=P) \cdot P(F=P)}{P(O|P=F)},~&amp;\textrm{Bayes Theorem}\\
            &amp;= \frac{1/2 \cdot 1/3}{1/2},~&amp;\textrm{Counting}\\
            &amp;= 1/3.\\
P(F\neq P|O, P) &amp;= \frac{P(O|F\neq P) \cdot P(F\neq P)}{P(O|P\neq F)},~&amp;\textrm{Bayes Theorem}\\
                &amp;= \frac{1 \cdot 2/3}{1},~&amp;\textrm{Counting}\\
                &amp;= 2/3.\end{split}\]</div>
<p>So from this we see that our first guess has a 2/3 chance of being wrong given the open door, so switching would
give us a 2/3 of being correct in that case. Additionally, by the Law of Total Probability, we could’ve computed
the chances of the first guess being correct (1/3) and taking the complement of that.</p>
<dl class="docutils">
<dt>Side-effect:</dt>
<dd>This code runs a simulation of the Monty Hall Problem to compute the probabilities and prints the
probability of being right when staying with the original choice or switching to the remaining door.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>num_samples</strong> (<em>int</em>) – the number of times to sample the distribution, must be positive.</li>
<li><strong>seed</strong> (<em>int</em>) – the random seed to ensure repeatability.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">None.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_6">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_6</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_6"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_6" title="Permalink to this definition">¶</a></dt>
<dd><p>Want to know if you can compute <span class="math notranslate nohighlight">\(P(H|e_1,e_2)\)</span> with different givens.</p>
<p>Let’s look at what this formula looks like after rearranging.</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(H|e_1,e_2) &amp;= \frac{P(e_1,e_2|H) \cdot P(H)}{P(e_1,e_2)},~&amp;\textrm{Bayes Thm.}\\
             &amp;= \frac{P(e_1|H) \cdot P(e_2|H) \cdot P(H)}{P(e_1,e_2)},~&amp;\textrm{Def. of Cond. Ind.}\\
             &amp;= \frac{P(e_1|H) \cdot P(e_2|H) \cdot P(H)}{\Sigma_h P(e_1,e_2|H) \cdot P(H)},~&amp;\textrm{Total Probability}\\
             &amp;= \frac{P(e_1|H) \cdot P(e_2|H) \cdot P(H)}{\Sigma_h P(e_1|H)\cdot P(e_2|H) \cdot P(H)},~&amp;\textrm{Def. of Cond. Ind.}\end{split}\]</div>
<ol class="lowerroman simple">
<li><span class="math notranslate nohighlight">\(P(e_1,e_2), P(H), P(e_1|H), P(e_2|H)\)</span>. This is sufficient from the second line above if we assume
independence between the <span class="math notranslate nohighlight">\(E\)</span> variables.</li>
<li><span class="math notranslate nohighlight">\(P(e_1,e_2), P(H), P(e_1,e_2|H)\)</span>. This is sufficient from the first line above, a single
applications of Bayes Theorem.</li>
<li><span class="math notranslate nohighlight">\(P(e_1|H), P(e_2|H), P(H)\)</span>. This is sufficient from the last line, after applying the Law of total
probability and Conditional Independence.</li>
</ol>
<p>So (ii) is the answer to part a), when we don’t know anything about the relationship between <span class="math notranslate nohighlight">\(E_1\)</span> and
<span class="math notranslate nohighlight">\(E_2\)</span>. All sets of givens are sufficient if we know the two variables are conditionally independent.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_7">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_7</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_7"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_7" title="Permalink to this definition">¶</a></dt>
<dd><p>Pairwise independence does not imply mutual independence.</p>
<p>Mutual independence means that <span class="math notranslate nohighlight">\(P(X_i|X_S) = P(X_i) \forall S \subseteq \{1,\ldots,n\}\setminus\{i\}\)</span>
and so the joint distribution of <span class="math notranslate nohighlight">\(P(X_{1:n}) = \prod_{i=1}^n P(X_i)\)</span>.</p>
<p>So it would be enough to show that for 3 variables that are all pairwise independent that they are
not mutually independent.</p>
<p>Consider a 5x5 grid where one variable <span class="math notranslate nohighlight">\((X_1)\)</span> is true only along the bottom 5 squares, another is true only
along the right side <span class="math notranslate nohighlight">\((X_2)\)</span>, and a third is true only along the main diagonal <span class="math notranslate nohighlight">\((X_3)\)</span>. The only overlap
any variable has with any other is in the lower right corner square.</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(X_1=T) &amp;= 5/25\\
P(X_1=F) &amp;= 20/25\\
P(X_1=T,X_2=T) &amp;= 1/25 = 5/25*5/25 = P(X_1=T)P(X_2=T)\\
P(X_1=T,X_2=F) &amp;= 4/25 = 5/25*20/25 = P(X_1=T)P(X_2=F)\\
P(X_1=F,X_2=T) &amp;= 4/25 = 20/25*5/25 = P(X_1=F)P(X_2=T)\\
P(X_1=F,X_2=F) &amp;= 16/25 = 20/25*20/25 = P(X_1=F)P(X_2=F)\\\end{split}\]</div>
<p>In this way, we see that each pair of variable is conditionally independent. The question is if they are
mutually independent. If they were, then <span class="math notranslate nohighlight">\(P(X_1,X_2,X_3) = P(X_1)P(X_2)P(X_3)\)</span>, but we see for
<span class="math notranslate nohighlight">\(P(X_1=T,X_2=T,X_3=T) = 1/25\)</span> (the lower right corner), but <span class="math notranslate nohighlight">\(P(X_1=T)P(X_2=T)P(X_3=T) = 1/125\)</span> so
we see that being pairwise conditionally independent does not imply mutual independence.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_8">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_8</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_8"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_8" title="Permalink to this definition">¶</a></dt>
<dd><p>Conditional independence iff joint factorizes.</p>
<p>Prove that <span class="math notranslate nohighlight">\(p(x,y|z)=g(x,z)h(y,z)~\textrm{iff}~X \perp Y | Z.\)</span></p>
<p>First, let <span class="math notranslate nohighlight">\(g(x,z) = p(x|z), h(y,z) = p(y|z)\)</span> since conditional probabilities
are functions of random variables these are permissible definitions of <span class="math notranslate nohighlight">\(g, h\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\textrm{The forward direction:}~X \perp Y | Z \Rightarrow p(x,y|z)=g(x,z)h(y,z).\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}p(x,y|z) &amp;= p(x|z)p(y|z),~&amp;\textrm{Def. of Cond. Ind.}\\
         &amp;= g(x,z)h(y,z),~&amp;\textrm{Defined above.}.\end{split}\]</div>
<p>Lemma: <span class="math notranslate nohighlight">\(p(x|y,z) = p(x|z)~\textrm{if}~X \perp Y | Z.\)</span></p>
<p>Proof:</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(x|y,z) &amp;= \frac{p(x,y,z)}{p(y,z)},~&amp;\textrm{Def. of Cond. Prob.}\\
         &amp;= \frac{p(x,y|z)p(z)}{p(y|z)p(z)}~&amp;\textrm{Def. of Cond. Prob.}\\
         &amp;= \frac{p(x|z)p(y|z)p(z)}{p(y|z)p(z)}~&amp;\textrm{Def. of Cond. Ind.}\\
         &amp;= p(x|z).\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\textrm{The reverse direction:}~p(x,y|z)=g(x,z)h(y,z) \Rightarrow X \perp Y | Z.\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}p(x,y|z) &amp;= \frac{p(x,y,z)}{p(z)},~&amp;\textrm{Def. of Cond. Prob.}\\
         &amp;= \frac{p(z)p(y|z)p(x|y,z)}{p(z)},~&amp;\textrm{Chain rule of prob.}\\
         &amp;= p(y|z)p(x|z),~&amp;\textrm{By the above lemma, Def. Cond. Ind.}\\
         &amp;= g(x,z)h(y,z),~&amp;\textrm{Defined above.}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="chapter.two_exercises.question_9">
<code class="descclassname">chapter.two_exercises.</code><code class="descname">question_9</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/chapter/two_exercises.html#question_9"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chapter.two_exercises.question_9" title="Permalink to this definition">¶</a></dt>
<dd><p>Conditional independence statements…</p>
<ol class="loweralpha">
<li><p class="first">Does <span class="math notranslate nohighlight">\((X \perp W|Z,Y) \wedge (X \perp Y|Z) \Rightarrow (X \perp Y,W|Z)\)</span>? Yes.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}p(X,Y,W|Z) &amp;= \frac{p(X,Y,W,Z)}{p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= \frac{p(X,W|Z,Y)p(Z,Y)}{p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= \frac{p(X|Z,Y)p(W|Z,Y)p(Z,Y)}{p(Z)},~&amp;\textrm{First given; Def. Cond. Ind.}\\
    &amp;= \frac{p(X,Z,Y)p(W|Z,Y)p(Z,Y)}{p(Z,Y)p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= \frac{p(X,Y|Z)p(Z)p(W|Z,Y)}{p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= p(X|Z)p(Y|Z)p(W|Z,Y),~&amp;\textrm{Second given; Def. Cond. Ind.}\\
    &amp;= \frac{p(X|Z)p(Y|Z)p(W,Z,Y)}{p(Z,Y)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= \frac{p(X|Z)p(Y|Z)p(Y,W|Z)p(Z)}{p(Z,Y)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= \frac{p(X|Z)p(Y,Z)p(Y,W|Z)p(Z)}{p(Z,Y)p(Z)},~&amp;\textrm{Def. Cond. Prob.}\\
    &amp;= p(X|Z)p(Y,W|Z).\end{split}\]</div>
</div></blockquote>
</li>
<li><p class="first">Does <span class="math notranslate nohighlight">\((X \perp Y|Z) \wedge (X \perp Y|W) \Rightarrow (X \perp Y|Z,W)?\)</span> No.</p>
<blockquote>
<div><p>If W and Z are describing the same event, then this is a true statement, but in general,
it fails. If we construct another discrete example using a 4x4 grid where X is true along
the bottom, Y is true along the right side, Z is true along the main diagonal and W is true
in the bottom right corner, the top left corner, and along the minor diagonal in the middle two
rows (not where Z is true), then we’ll have a contradiction. We get the first two statements as
being true, <span class="math notranslate nohighlight">\((X \perp Y |Z) \wedge (X \perp Y|W)\)</span>, but we’ll find that <span class="math notranslate nohighlight">\(p(X|W,Z) = p(Y|W,Z) = 1/2\)</span>
while <span class="math notranslate nohighlight">\(p(X,Y|W,Z) = 1/2\)</span> not 1/4, giving us a contradiction and allowing us to say that the
result is not true.</p>
</div></blockquote>
</li>
</ol>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">None.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-chapter">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-chapter" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="modules.html" class="btn btn-neutral" title="chapter" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Steven Loscalzo.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.0.1',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>